\chapter*{Conclusion}
So far it looks that our method works if we set threshold for cooperation to smaller value (cca 70 shared reward) and look for 0.30 quantil.

KL divergence as additional partial reward seems to be reasonable exploration ehancing technique that did not have that significant negative effect on trained policy.
But when applied in objective function it seems to force the learned policy the way that is worsening the main objective.

TODO: Even when no diversification was applied, it managed in some cases to learn to cooperate with some self-play agents that none from pop init could.

While populatino agents are more cooperative at lower level, there are still quite a lot self-play agents that our trained agents cannot cooperate with.


{\color{blue}TODO: what should go in here?}

\addcontentsline{toc}{chapter}{Conclusion}
