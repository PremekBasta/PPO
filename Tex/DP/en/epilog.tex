\chapter*{Conclusion}

We started this thesis by briefly introducing the reinforcement learning and its extenstion to multi-agent environments (Chapters \ref{IntroductionChapter}, \ref{RLChapter} and \ref{MAS}).

We then progressed with providing description of simplified fully cooperative multi-agent cooking game environment (Chapter \ref{OvercookedChapter}) which we worked with to study the cooperative capabilites of trained agents. 
Subsequently, we tried to reimplement few results based on related previous work (Chapters \ref{RelatedWorkChapter} and \ref{BackgroundChapter}), where we attempted to demonstrate allegedly poor cooperative abilities of self-play trained agents, which later became a solid baselines for our experiments.

We later introduced our own novel approach for diverse population building method (Chapter \ref{DiversePopulationMethodChapter}).
Here we tried to simplify the multi-agent environment towards single-agent perspective by effectively introducing domain randomization technique.
Apart from this randomization we tried to induce even more diversity into the population by incorporating KL divergence bonuses to enhance the differences between the population agents.
With such designed settings we executed several different experiments to evaluate cooperative capabilites and robustness of such method. 

Here we revisited the problem of evaluation of cooperation, where we demonstrated that taking into consideration only an overall mean is not sufficient metric.
Instead we proposed to first perform cross-play evaluation of evaluated agents against evaluation set and then order these results in an ascending order and study the whole progression of such ordering.

With the addition of considering lower 0.15 quantile value of results we were able to fulfill the goals that we have set for ourselfs in the beginning and demonstrated robust cooperative abilities with much wider set of agents compared to self-play agents.
However, while our agents were able to cooperate with significantly wider set of agents, this came with a cost of decreased level of cooperation as our agents were not able to reach such high outcomes as self-play agents.
Nevertheless we believe that such shift can be considered as an improvement in cooperation abilities since we believe it is more desirable to have an agent capable of decent cooperation with much wider types of agents behaviors then having a specialized agent who can reach outstanding results only with limited type of partner behavior.

We were able to improve the cooperation of agents slightly even further by extending the information available to the agents by incorporation the temporal information about previous states of the environment.

With a




So far it looks that our method works if we set threshold for cooperation to smaller value (cca 70 shared reward) and look for 0.30 quantil.


TODO: Even when no diversification was applied, it managed in some cases to learn to cooperate with some self-play agents that none from pop init could.

While populatino agents are more cooperative at lower level, there are still quite a lot self-play agents that our trained agents cannot cooperate with.
Lack of prioritization of population sampling


{\color{blue}TODO: what should go in here?}

\addcontentsline{toc}{chapter}{Conclusion}
