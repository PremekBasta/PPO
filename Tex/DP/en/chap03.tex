\chapter{Multi agent environments for RL ???}

In this chapter we revisit concepts from previous section and extend them to multi-agent settings.
We introduce theoretical definitions to various settings type and provide possible schemes for concurrent learning of multiple agetns.
And at the end we mention two popular RL approaches in the field of MARL. 

\section{Multi-agent Markov Decision Process}
So far, all of the theory and algorithms that have been built have revolved around environments where there is a single agent.
However, this is rarely the case in real-world problems.
Much more often we encounter environments with multiple agents operating within them.
In the general case, the agents are heterogeneous, which means that the agents may have different goals.
Nevertheless, we will mostly focus on environments that are fully cooperative, meaning that the utility of any particular state of the system is equivalent for all agents.

With this setting we can extend the definition from MDP section \ref{MDP}:
\linebreak
\textbf{Multiagent Markov Decision Process}(\cite{MMDP})is a tuple 
$\langle n, S, \mathcal{A}, T, R\rangle$
\begin{itemize}
    \item $n$ is the number of agents
    \item $S$ is the set of all valid states,
    \item $\mathcal{A} $ is the set of joint actions
    \item $T: S \times \mathcal{A} \times S \rightarrow [0,1]$ is the transition function
    \item $R: S  \rightarrow \mathbb{R}$ is a real-valued reward function, where reward determined by the reward function $R(s)$ is received by the entire collection of agents, or alternatively, all agents receive the same reward.

\end{itemize}

\section{Decentralized Partially Observable MDP}
Unfortunately, we cannot stop with this definition, because the MMDP model provides a description that is far too ideal for real-world problems.
The MMDP model expects the state of the environment to be globally available to all agents.
However, in multi-agent environments this is rarely the case.
Usually, the world can be of high complexity and combined with limited sensory capabilities, the agent is able to perceive only limited observations that describe only a part of the entire environment state.
Therefore, we extend our model definition to include the concept of partial observability.

\textbf{Decentralized partially observable Markov decision process} (Dec-POMDP, \cite{DecPOMDP}) is tuple
$\langle n, S, \mathcal{A}, P, R, \mathcal{O}, O, h, b^0\rangle$ where,
\begin{itemize}
    \item $n$ is the number of agents
    \item $S$ is the set of all valid states,
    \item $\mathcal{A} $ is the set of joint actions
    \item $P: S \times \mathcal{A} \times S \rightarrow [0,1]$ is the transition function
    \item $R: S  \rightarrow \mathbb{R}$ is the immediate reward function
    \item $\mathcal{O}$ is the set of joint observations
    \item $O: \mathcal{A} \times S \rightarrow \mathcal{P}(\mathcal{O})$ is the observation function
    \item $h$ is the horizon of the problem
    \item $b^0 \in \mathcal{P}(S)$, is the initial state distribution at time $t=0$

\end{itemize}

We define $\mathcal{A}  = \times_i \mathcal{A}^i$, where $\mathcal{A}^i$ is the set of actions available to agent $i$.
Similarly $\mathcal{O} = \times_i \mathcal{O}^i$, where $\mathcal{O}^i$ is the set of observations available to agent $i$. 
Notice here, that even this definition extension did not provide us with model that is capable of describing environment where agents have different reward function, which is needed for situation where agents have different goals or are even competetive.
This extension is possible with definition of Stochastic game. 
However, we do not need to 

When the observation fulfils the condition that the individual observation of all agents uniquely identify the true state of the environment, environment is considered fully observable and such Dec-POMDP can be reduced to MMDP.
\subsection*{Notation}
To denote joint entities we will be using bold case:
$\boldsymbol{a}=(a^1,..a^n) \in \mathcal{A}$.
Joint policy $\boldsymbol{\pi}$ induced by the set of individual policies ${\{\pi^i\}}_{i \in n}$  gives the mapping from states into joint actions.
Now the similar notation as in first chapter may be used using the bold symbols:

\begin{align*}    
        \textrm{Trajectory}&: \quad \boldsymbol{\tau} = (s_0, \boldsymbol{a_0}, s_1, \boldsymbol{a_1}, ...) \\
        \textrm{Return}&: \quad R(\boldsymbol{\tau}) = \sum_{t=0}^{\boldsymbol{\tau}}r_t  \\
        \textrm{Probability of trajectory}&: \quad P(\boldsymbol{\tau}|\boldsymbol{\pi}) = \rho_0(s_0) \prod_{t=0}^{|\boldsymbol{\tau}|} P(s_{t+1}|s_t,\boldsymbol{a_t})\boldsymbol{\pi}(\boldsymbol{a_t}|s_t) \\
        \textrm{Expected return}&: \quad J(\boldsymbol{\pi})=\int_{\boldsymbol{\tau}} P(\boldsymbol{\tau}|\boldsymbol{\pi})R(\boldsymbol{\tau})= \mathop{\mathbb{E}}_{\boldsymbol{\tau} \sim \boldsymbol{\pi}}[R(\boldsymbol{\tau})]
\end{align*}


Given the joint utility funtion, it is useful to think of the collection of agents as single agent whose goal is to produce optimal joint policy.
The problem with treating MMDP as a standard MDP where actions are distributed lies in coordination.
In general there exist multiple different optimal joint policies.
However, even when all agents choose their individual policy according to some optimal policy, there is no guarantee that they all select from the same optimal joint policy.
Such final joint policy may be nowhere near the optimal one and most likely, even produce significantly worse performance.
In theory, there are two simple ways how to ensure optimal coordination.
Firstly, there can exist some central control mechanism \ref{CentralScheme}, that can compute joint policy and then instruct it to all individual agents.
Or secondly, each agent may comunicate its choice of individual policy to others.
However, both approaches are in general case unfeasible.

\subsection{Nash Equilibria}
Alternatively we can look at the MMDP from the point of n-person game. 
Then the problem of determining an optimal joint policy can be viewed as a problem of optimum equilibirum selection.
Nash equilibrium (\cite{NASH}) for a $\boldsymbol{\pi}^*$ can be defined as:\linebreak
Set of policies $\pi^*$ is a Nash equilibrium if:
\[
    \forall i \in n, \forall \pi^i: J(\boldsymbol{\pi}^{*-i}, \pi^{*i}) \ge J(\boldsymbol{\pi}^{*-i}, \pi^i)
\]

However, not all Nash equilibria correspond to optimal joint policy as some may have smaller utility value then others.
This causes multi-agent environments to being more sensitive to converging to local suboptimal policies as only way to escape from such equilibrium is through coordinated change of all individual policies.




\section{Learning schemes}
\subsection*{Centralized scheme}\label{CentralScheme}
From theoretical point of view, we could look at MMDP as being instance of single-agent MDP where the goal would be to learn one central joint policy which would then be distributed among individual agents.
With this idea we could solve MMDP problems using same single-agent RL algorithms from second chapter only instead of learning single policy we would learn joint policy.
However, this approach has several flaws in real-world cases.

Firstly, from the practical point of view, agents in this scenario would be in a sense passive entities whose job would be only to report perceived observations to some central authority.
After all observations have been collected by the central unit, a joint policy is produced and distributed to the agents, who then blindly act as directed by the central control.
This has several serious problems.
Let us imagine that some failure of the central mechanism occurs.
Suddenly, the whole system of agents collapses because all agents lack individual autonomy.

Secondly, such intensive communication between all agents and the central control mechanism may be too demanding or may not be even plausible due to technical reasons.

And lastly from the theoretical point of view, representation of such joint policy grows exponentially with number of agents with respect to both observations ($\prod_{i=0}^{n}{|\mathcal{O}^i|}$)
and actions ($\prod_{i=0}^{n}|\mathcal{A}^i|$), which makes it unscalable.

\subsection*{Concurrent scheme}
On the other side of the spectrum lies the concurrent scheme.
Here all kinds of global informations are ommitted and agents solely rely on their local observations.
Such training of a agent is then completely independent.

\subsection*{Centralized training with decentralized execution}
And lastly somewhere in between the concurrent and centralized scheme we can identify so called centralized training with decentralized execution.
Here we look at the two life stages of agents. 
We firstly teach our agents in safe laboratory conditions, and only after training is finished they are deployed in real world conditions.

During the training, we can take advantage of the fact that we are likely to have more information about the state of the environment at our disposal.
Whether that is true global state of the environment, local observations of other agents or actions taken by others.
All of this additional information can be incorporated into the training process to capture the true state to act upon.
In other words we want to make the training process as simple and exact as possible.

Once the agents are deployed they are once again dependent solely on their local observation.

This learning scheme is quite often utilized in RL algorithms, where both actor and critic is utilized.
It is demonstrated for example in aformentiond PPO \ref{PPO} algorithms.
During training both actor and critic are being trained concurrently and the value estimate obtained by the critic can be utilized to give more accurate information about true value of the given state to make the actor policy update more stable and exact.
Once the training is complete, the agents that are given local observations are asked to take action based solely on the actor.

\section{Non-stationarity}
Besides all already mentioned problems connected with partial observability, local equilibiria and policy distribution,
there is one more important problem that we did not have to deal with in single-agent settings.
Even though we are capable of reproducing the value function expressions:
\begin{align*}  
    &V^{\boldsymbol{\pi}}(s) = \mathop{\mathbb{E}}_{\boldsymbol{\tau} \sim \boldsymbol{\pi}}[R(\boldsymbol{\tau})|s_0=s] \\
    &Q^{\boldsymbol{\pi}}(s,\boldsymbol{a}) = \mathop{\mathbb{E}}_{\boldsymbol{\tau} \sim \boldsymbol{\pi}}[R(\boldsymbol{\tau})|s_0=s,\boldsymbol{a_0}=\boldsymbol{a}] \\ 
    &A^{\boldsymbol{\pi}}(s,\boldsymbol{a}) = Q^{\boldsymbol{\pi}}(s,\boldsymbol{a}) - V^{\boldsymbol{\pi}}(s)
\end{align*}
due to the non-stationarity problem, we cannot obtain optimal value using Bellman equations as it was done in single-agent setting.
Non-stationarity refers to the fact that changing policies of other agents as a consequence changes, from a fixed agent's perspective, state transition and reward function of the environment. 
Using the idea of Bellman equations for optimality is still possible.
However, in multi-agent setting we lose the theoretical foundation promising convergence for optimal policy, and training using the Q-learning update rule must be acompanied by some mechanisms to overcome non-stationarity problem.

\section{RL algorithms}
Having extended our mathematical model to settings that satisfy the conditions for a multi-agent environment, we can continue with a brief mention of the multi-agent variants of reinforcement learning algorithms discussed in the previous chapter.
\subsection*{MADDPG}
First of the mentioned algorithms is Multi Agent Deep Deterministic Policy Gradient (MADDPG, \cite{MADDPG}) algorithm.
Authors extend DDPG (mentioned briefly in \ref{QPlusPolicy}) by using centralized learning with decentralized execution.
To combat the problem of non-stationarity, authors propose sampling from the ensemble of policies for each agent to obtain more robust multi-agent policies.
This algorithm has been widely experimented on academicaly with respect to multi-agent coordination environments.
\subsection*{MAPPO}
Lastly we want to mention the recent success in the form of Multi Agent Proximal Policy Optimization (MAPPO, \cite{MAPPO}) variant of PPO \ref{PPO} algorithm.
Authors revisit the usage of PPO in multi-agent settings.
In recent years, MADDPG was usually the first choice of an algorithm when dealing with multi-agent environments, leaving PPO ommited.
Authors hypothesize, this was due to the two reasons, firstly, belief that PPO is less sample-efficient than off-policy methods and secondly, the fact that common implementation and hyperparameters practices when using PPO in single-agent settings often do not yield strong performance when transfered to multi-agent settings.

Authors propose five important changes with regard to the multi-agent settings.
TODO:
\begin{itemize}
    \item Value normalization: 
    \item Input Representation to Value Function:
    \item Training Data Usage:
    \item PPO Clipping:
    \item PPO Batch Size:
\end{itemize}

Utilizing these five concepts, they were able, without any other domain or structural modification, to achieve comparable or superiror results compared to off-policy algorithms on several benchmark frameworks.   





