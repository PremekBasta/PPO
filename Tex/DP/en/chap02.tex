\chapter{RL algorithms}
\section{Q-Learning}

\subsection*{Idea}
We will start with family of algorithms that focuses on learning action-value approximator $Q_\theta(s,a)$ as described in previous chapter.
For this reason, the group of such algorithms can be also refered to as Q-learning.
Our primary goal in RL problem is to find policy that agent can follow.
In the case of Q-learning once we have have learned approximator $Q_\theta(s,a)$ we can derive policy by always taking the best possible action in given state according to the learned action-value function
\[a(s) = \arg \max_a Q_\theta(s,a).\]
By incorporating Bellman equations for optimal policy we can directly train Q-network by minimizing the loss
\[L(\Theta)=(r + \gamma \max_{a'} Q_\theta(s',a') - Q_\theta(s,a))^2.\]
And by computing loss gradient we arrive at update rule
\[Q_\theta(s,a) = Q_\theta(s,a) + \alpha (r + \gamma \max_{a'}Q_\theta(s',a') - Q(s,a)),\]
which is the backbone of the algorithm \ref*{Qlearning} bearing the same name.
\subsection*{Instability}
Algorithm has been rarely used in his pure form. 
This aproach has been primarily described for tabular methods where action-value function is represented by table instead of network approximator.
In its simplest form, the training is unstable and suffers from a number of significant shortcomings.
Most worthy of mention is the theoretical deadly triad counter example \cite{sutton2018reinforcement} which consists of a combination of value approximation, bootstrapping, and off-policy training that can lead to instability and divergence.
Condition of value approximation is met since we use Q-network to approximate action-value.
Bootstrapping means that estimate is used for computation of targets, this is also true in Q-learning for the same reason.
Lastly term off-policy stands for approach where training data are collected using different distribution than that of a target policy.





\begin{algorithm}
  \KwIn{initial action-value aproximator Q parameters $\theta$ .}
  \Repeat{convegence}{
    Observe state $s$ and select action $a$ according to $\epsilon$-greedy w.r.t. $Q$ e.g.
    \[
      a =
      \begin{cases}
        \text{random action},      & \text{with probability} \; \epsilon, \\
        \underset{a}{\arg\max} Q(s,a), & \text{otherwise}.
      \end{cases}
    \] \\
    Execute $a$ in the environment. \\
    Observe next state $s'$, reward  $r$ and done signal $d$ to indicate whether $s'$ is terminal. \\
    \If{$d$ is true}{
      Reset environment state. \\
    }
    Compute targets \\
    \[y(r,s',d) = r + \gamma(1-d) \max_{a'}Q_\theta(s',a')\]

    Update Q-network taking one step of gradient decent on \[(y(r,s',d) - Q_\theta(s', a))^2\]
  }
  \caption{Q-learning}
  \label{Qlearning}
\end{algorithm}






\subsection*{DQN}
One of the most outstanding paper based on Q-learning was the algorithm Deep Q-learning \ref{DeepQLearning} which demonstrated super-human results on multiple Atari games \cite{Atari}.
We give the pseudocode of the algorithm in it's original form.
Notation might seem a bit different from ours, nevertheless it represents the same mechanisms that we expect.
To address the problem of correlated transition sequences of data they introduce experience replay where previously sampled transitions are stored.
During training data are sampled from this buffer thus smoothing the training distribution over different past behaviors.
However, probably the most important idea was the usage of target Q-network, which broke value approximation condition of deadly triad, thus making the algorithm more stable.
Target network is a copy of original Q-network that has its parameters frozen and updated only once in while based on parameters of the main network.
It's sole purpose is to compute target estimates that are not directly dependent on Q-network function.

\begin{algorithm}
  Initialize replay memory $D$ to capacity $N$ \\
  Initialize action-value function $Q$ with random weights $\theta$ \\
  Initialize target action-value function $\hat{Q}$ with weights $\theta^- = \theta $\\
  
  \For{episode = 1,M }{
    Initialize sequence $s_1=\{x_1\}$ and preprocessed sequence $\phi_1=\phi(s_1)$ 
    \For{t=1, T}{
      With probability $\epsilon$ select a random action $a_t$ \\
      otherwise select $a_t = \underset{a}{\arg\max} Q(\phi(s_t),a; \theta)$

      Execute action $a_t$ in emulator and observer reward $r_t$ and image $x_{t+1}$

      Set $s_{t+1}=s_t,a_t,x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$

      Store transition $(\phi_t,a_t, r_t, \phi_{t+1})$ in $D$

      Sample random minibatch of transitions $(\phi_j,a_j,r_j,\phi_{j+1})$ from $D$

      Set
      \[
        y_j =
        \begin{cases}
          r_j      & \text{if terminates at step }  j+1  \\
          r_j + \gamma \max_{a'}\hat{Q}(\phi_{j+1}, a'; \theta^-)     & \text{otherwise} 
        \end{cases}
      \]

      Perform a gradient descent step on $(y_j - Q(\phi_j,a_j; \theta))^2$ with respect to the network parameters $\theta$

      Every $C$ steps reset $\hat{Q}=Q$

    }
  }
  \caption{Deep Q-learning with experience replay}
  \label{DeepQLearning}
\end{algorithm}

\subsection*{Rainbow}
Deep Q Learning was a significant contribution that led to the study of further Q Learning capabilities.
Project Rainbow \cite{Rainbow} could be probably marked as peak of such research.
In this paper they examine further several isolated ideas of possible improvements and try to combine them together.
To name a few, they use Double Q-network to address the problem of maximization bias and improves sampling from experience buffer by considering priority of stored individual data samples.
Together with all other improvements that achieved at given time state-of-the-art performance on Atari 2600 benchmark, both in terms of data efficiency and final performance.








\section{Policy gradient methods}
\subsection{Idea}
In the previous section, we got acquainted with the first group of RL algorithms, where we derived final policy by taking action according to $argmax$ of our Q function approximator.
Since our objective is to find optimal policy, using this approach of considering Q values may seem a bit indirect.
There is a whole other family of algorithms out there that deal with this very issue.
As the name suggests Policy gradient algorithms focus on directly optimizing policy $\pi_\theta(a|s)$. 
This is achieved by directly taking steps along gradient of the performance objective of expected return $J(\pi_\theta) = \mathop{\mathbb{E}}_{\tau \sim \pi_\theta}[R(\tau)]$.
Optimization step then has the form of:
\[\theta_{k+1} = \theta_k+\alpha  \nabla_\theta J(\pi_\theta)|\theta_k\] and the gradient $\nabla_\theta J(\pi_\theta)$ is called \textbf{policy gradient}.

Before we can transform this into an algorithm we have to be able to compute policy gradient numerically.
Such expression can be obtained as result of Policy Gradient Theorem (PGT).

\par
TODO: Following algorithm seems to be inspired by \url{https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html} and references original literature where no proof for integral is included.
Also it already includes baseline b(s) which was not stated, nor proved as a part of original theorem

\subsection{Policy Gradient Theorem}
Policy Gradient Theorem(\cite{sutton2018reinforcement}). It holds: 
\begin{align*}
  % \nabla_\theta J(\pi_\theta) \propto \mathop{\mathbb{E}}_{\tau \sim \pi_\theta}[\sum_{t=0}^{|\tau|}R(\tau) \nabla_\theta \log \pi_\theta(a_t|s_t)] \\
  %  \propto \mathop{\mathbb{E}}_{s_t \sim \eta_{\pi_\theta}}  \mathop{\mathbb{E}}_{a_t \sim \pi_\theta}  [R(\tau) \nabla_\theta \log \pi_\theta(a_t|s_t)]
  \nabla_\theta J(\pi_\theta) = \mathop{\mathbb{E}}_{\tau \sim \pi_\theta}[\sum_{t=0}^{|\tau|} \nabla_\theta \log \pi_\theta(a_t|s_t) R(\tau)] \\
\end{align*}
This can be proven by rewriting the formula in the following way:
\begin{align*}
  \nabla_\theta J(\pi_\theta) &= \nabla_\theta \mathop{\mathbb{E}}_{\tau \sim \pi_\theta}[R(\tau)] \\
  &= \nabla_\theta \int_{\tau}^{}P(\tau|\theta)R(\tau) \\
  &= \int_{\tau}^{} \nabla_\theta P(\tau|\theta)R(\tau) \\
  &= \int_{\tau}^{} P(\tau|\theta)\nabla_\theta \log P(\tau|\theta)R(\tau) \\
  &= \mathop{\mathbb{E}}_{\tau \sim \pi_\theta}[\nabla_\theta \log P(\tau|\theta)R(\tau)] \\
  &= \mathop{\mathbb{E}}_{\tau \sim \pi_\theta}[\sum_{t=0}^{|\tau|}  \nabla_\theta \log \pi_\theta(a_t|s_t) R(\tau)] \\
  % = \mathop{\mathbb{E}}_{\tau \sim \pi_\theta}[\sum_{t=0}^{|\tau|} R(\tau)\nabla_\theta \log \pi_\theta(a_t|s_t) ]     
\end{align*}

It is interesting to look at the expression and what does it represent. 
There are two important part of the expression $\pi_\theta(a_t|s_t)$ and $R(\tau)$.
If we take gradient step of this objective we say we want to make change of the log probability $\pi_\theta(a_t|s_t)$ weighted by how good the expected return was.
However this may feel somehow counter intuitive since excepted return considers all rewards of the episode.
We might want to limit ourselfs only to the future consequences of given action.
Fortunatly it can be shown that $R(\tau)$ can be replaced by many other useful functions(\cite{GAE}):

\[
  \nabla_\theta J(\pi_\theta) = \mathop{\mathbb{E}}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{|\tau|} \Psi_t \nabla_\theta \log \pi_\theta(a_t|s_t)\right],
\]
where $\Psi_t$, can be one of the following:
\begin{list}{}{}
  \item $\sum_{t=0}^{|\tau|} r_t$: total reward of the trajectory
  \item $\sum_{t=t'}^{|\tau|} r_t$: reward following action $a_t$
  \item $\sum_{t=t'}^{|\tau|} r_t - b(s_t)$: baselined version of previous formula
  \item $Q^\pi(s_t,a_t)$: state-action value function
  \item $A^\pi(s_t,a_t)$: advantage function
\end{list}

\subsection{Vanilla Policy Gradient}
Common practise for policy gradient algorithms is utilize some form of advantage function where baseline function is using value aproximator $b(s_t) = V^\pi(s_t)$.
Value approximator is usually represented by another neural network and is being learned concurrently with the policy.
There exists a naming convenction for the two types of network.
The policy network is usually called an actor because it's job is to provide a policy that the agents act upon.
The value network, on the other hand, is often recalled as a critic, since it in a sense produces a critique of the value of the state.
Incorporating value approximator and idea of advantage function reduces variance in sample estimation and results in more stable and faster learning.

With all of this being said we present first algorithm from this class.

\begin{algorithm}[H]
  Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
  
  \For{k = 0,1,2,... }{
      Collect set of trajectories $D_k=\{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.

      Compute rewards-to-go $\hat{R}$.

      Compute advantage estimates, $\hat{A_t}$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.

      Estimate policy gradient as 
      \[
        \hat{g_k} = \frac{1}{|\mathcal{D}_k|} \sum_{\tau \in \mathcal{D}_k}^{}\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t)|_{\theta_k}\hat{A_t}.
        \]

      Compute policy update, either using standard gradient ascent,
      \[\theta_{l+1} = \theta_k + \alpha_k \hat{g_k},\]
      or via another gradient ascent algorithm like Adam.
      Fit value function by regression on mean-squared error:

      \[
        \phi_{k+1} = \arg \min_\phi \frac{1}{|\mathcal{D}_k|T}\sum_{\tau \in \mathcal{D}_k}^{}\sum_{t=0}^{T}(V_\phi(s_t) - \hat{R_t})^2,
        \]
      typically via some gradient descent algorithm.


  }
  
  \caption{Vanilla Policy Gradient Algorithm}
  \label{Vanilla}
\end{algorithm}

Policy gradient methods differ from Q-learning in several aspects.

Firstly the absolute value of performance objective function we optimize cannot be interpreted in terms of performace result.
Taking one step of gradient descent does not even guarantee improving expected return in general.
On a given batch of samples value of $-\infty$ can be achieved. 
However expected return of changed policy would most likely be abysmal.

And secondly, policy gradient algorithms are \textbf{on-policy}, meaning that only data samples collected using most recent policy are used for training.
Oppositely to Q-learning off-policy approach where training data samples collected throughout entire training process are used for learning steps.
For this reason policy gradient algorithms, especially Vanilla Policy Gradient algorithm is often considered as sample inefficient in comparision to off-policy algorithms.


\subsection{Trust Region Policy Optimization}
Although standard policy gradient step makes small policy change within the parameter space, it, as it turns out, might have a singificant impact on performance difference.
Therefore, the vanilla policy gradient algorithm must be careful not to take large steps, making it an even more sample-inefficient algorithm.

Next algorithm from the family of policy gradient theorem is trying to face this problem.
As the name suggests, Trust Region Policy Optimization (\cite{TRPO}) attempts to take steps within the region where it is constrained so as not to degrade performance.

TRPO proposes theoretical update of parametrized policy $\pi_\theta$ as 
\begin{align*}
  \theta_{k+1} = \arg &\max_\theta \mathcal{L}(\theta_k,\theta)   \\  
  &\text{s.t.}  \bar{D}_{KL}(\theta||\theta_k) \leq \delta
\end{align*}

where \[\mathcal{L}(\theta_k,\theta) = \mathop{\mathbb{E}}_{s,a \sim \pi_{\theta_k}} \ \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}} (s,a) \right] \]
is surrogate advantage measuring policy $\pi_\theta$ peformance relative to the old policy $\pi_{\theta_k}$.

And \[\bar{D}_{KL}(\theta||\theta_k) = \mathop{\mathbb{E}}_{s \sim \pi_{\theta_k}}[D_{KL}(\pi_\theta(\cdot|s)||\pi_{\theta_k}(\cdot|s))]\] is average KL-divergence(\cite{KLDIV}) between policies evaluated on states visited by the old policy.
However working with theoretical update of TRPO in this form is not an easy task. 
Therefore approximations obtained by applying Taylor expansion around $\theta_k$ are being used:

\begin{align*}
  \mathcal{L}(\theta_k,\theta) &\approx g^T(\theta - \theta_k) \\
  \bar{D}_{KL}(\theta||\theta_k) &\approx \frac{1}{2} (\theta-\theta_k)^TH(\theta-\theta_k)
\end{align*}

And original problem can be then reformulated as approximate optimization problem:

\begin{align*}
  \theta_{k+1} = \arg &\max_\theta g^T(\theta - \theta_k)   \\  
  &\text{s.t.}  \frac{1}{2} (\theta-\theta_k)^TH(\theta-\theta_k) \leq \delta
\end{align*}
Such approximate reformulation can be solved analytically using methods of Lagrangian duality(\cite{LagrangDuality}), yielding solution:
\[\theta_{k+1} = \theta_k + \sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g\]
However, since we used Taylor expansion, approximation error might break KL divergence constraint.
For this reason TRPO incorporate idea of backtracking line search (\cite{BacktrackingLineSearch}):
\[\theta_{k+1} = \theta_k + \alpha^j \sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g\]
where $\alpha \in (0,1)$ is the backtracking coefficient and $j$ is smallest non negative integer such that KL divergence constraint is satisfied and surrogate advantage is positive.


\begin{algorithm}[H]
  Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$

  Hyperparameters: KL-divergence limit $\delta$, backtracking coefficient $\alpha$, maximum number of backtracking steps $K$
  
  \For{k = 0,1,2,... }{
      Collect set of trajectories $D_k=\{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.

      Compute rewards-to-go $\hat{R}$.

      Compute advantage estimates, $\hat{A_t}$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.

      Estimate policy gradient as 
      \[
        \hat{g_k} = \frac{1}{|D_k|} \sum_{\tau \in D_k}^{}\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t)|_{\theta_k}\hat{A_t}.
        \]

      Use the conjugate gradient algorithm to compute 
      \[
      \hat{x_k} \approx \hat{H}_k^{-1} \hat{g_k},
      \]
      where $\hat{H}_k^{-1}$ is the Hessian of the sample average KL-divergence.

      Compute the policy by backtracking line search with 
      \[\theta_{l+1} = \theta_k + \alpha^j \sqrt{\frac{2\delta}{\hat{x}_k^T \hat{H}_k \hat{x}_k}} \hat{x}_k, \]
      where $j \in \{0,1,2, ...K\}$ is the smallest value which improves the sample loss and satisfies the sample KL-divergence constraint.
      

      Fit value function by regression on mean-squared error:
      \[
        \phi_{k+1} = \arg \min_\phi \frac{1}{|\mathcal{D}_k|T}\sum_{\tau \in \mathcal{D}_k}^{}\sum_{t=0}^{T}(V_\phi(s_t) - \hat{R_t})^2,
        \]
      typically via some gradient descent algorithm.


  }
  
  \caption{Trust Region Policy Optimization}
  \label{TRPO}
\end{algorithm}


\subsection{Proximal Policy Optimization}\label{PPO}
Finally we will conclude this chapter with last algorithm from family of policy gradient algorithms that is considered in many aspects to be the state of art algorithm.
As the authors of this next algorithm state. (TODO: should I emphasize more quoting the original text?)
With the leading contenders Q-learning, "vanilla" policy gradient methods and trust region policy gradient methods there is still room for developing a method that is 
\begin{itemize}{}{}
  \item scalable - large models and parallel implementations
  \item data efficient
  \item robust - successful on a variety of problems without Hyperparameters tuning
\end{itemize}

Q-learning is poorly understood and fails on many simple problems.
Vanilla policy gradient methods have poor data effiency and robustness.
And trust region policy optimization is relatively complicated, and is not compatible with architectures that include noise or parameter sharing.

Proximal policy optimization (\cite{PPO}) aims at data efficiency and realiability of TRPO performance, while using only first-order optimization.
Authors propose a novel objective with clipped probability ratios, which forms a pessimistic lower bound of the performance of the policy.

Let $r_t(\theta)$ denote the probability ratio 
\[
  r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}.
\]
Then the "surrogate" objective of TRPO can be expressed again in the form:
\[
  L^{CPI}(\theta)=\hat{\mathbb{E}}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat{A_t} \right] = \hat{\mathbb{E}}_t \left[ r_t(\theta) \hat{A_t}\right]   
\]

CPI refers to conservative policy iteration(\cite{CPI}), where was this objective originaly proposed.
Maximization of $L^{CPI}$ without any constraint would lead, as it was already discussed in previous section, to an excessively large policy update.
Hence authors propose new modified objective called clipped surrogate objective that penalizes changes to the policy that move $r_t(\theta)$ far away from 1.

\[
  L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min (r_t(\theta) \hat{A_t}, \textrm{clip} (r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A_t})\right],
\]
where the value $\epsilon = 0.2$ is empirically suggested. 
Objective can be intuitively explained as follows.
First term in $\min$ is $L^{CPI}$ as before.
And the second term modifies surrogate objective by clipping probability ratio which prevents $r_t$ from escaping the interval $[1-\epsilon, 1+ \epsilon]$.
Finally taking the minimum of the clipped and unclipped objective makes the final objective pessimistic bound on the unclipped objective.

Training process of PPO then proposes alternating between sampling data from the policy and then performing several epochs of optimization steps on the sampled data.
Notice here that for every first training epoch it holds that $r_t(\theta) = 1$ making the objective in first epoch always equal to $L^{CPI}$

Alternatively authors propose second version of PPO incorporating KL penalty as part of objective, which makes it even more similar to idea proposed in TRPO.
However, we will not cover here more details as authors themselfs uplifts more the version including clipping of surrogate objective.

If using neural network architecture that shares parameters between policy and value function, a combined objective function that contains both the policy surrogate and value function error term must be applied.
\[
  L_t^{CLIP+VF}(\theta) =   \hat{\mathbb{E}}_t \left[L_t^{CLIP}(\theta) - c_1 L_t^{VF}(\theta) \right],
\]
where $c_1$ is coefficient nad $L_t^{VF}$ is a squared-error loss $(V_\theta(s_t) - V_t^{targ})^2$.

\begin{algorithm}[]
  Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
  
  \For{k = 0,1,2,... }{
    Collect set of trajectories $D_k=\{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.

    Compute rewards-to-go $\hat{R}_t$.

    Compute advantage estimates, $\hat{A_t}$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.

    Update the policy by maximizing the PPO-Clip objective:
    \[
      \theta_{k+1} = \arg \max_\theta \frac{1}{|\mathcal{D}_k|T}\sum_{\tau \in \mathcal{D}_k}\sum_{t=0}^{T}\min \left(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_k}(a_t|s_t)} A^{\pi_{\theta_k}}(s_t,a_t), g(\epsilon, A^{\pi_{\theta_k}}(s_t,a_t))\right),
      \]
    typically via stochastic gradient ascent with Adam.

    Fit value function by regression on mean-squared error:
    \[
      \phi_{k+1} = \arg \min_\phi \frac{1}{|\mathcal{D}_k|T}\sum_{\tau \in \mathcal{D}_k}^{}\sum_{t=0}^{T}(V_\phi(s_t) - \hat{R_t})^2,
      \]
    typically via some gradient descent algorithm.


  }
  
  \caption{Proximal Policy Optimization}
  \label{PPO}
\end{algorithm}

\pagebreak
Lastly one thing, that we explicitly haven't covered yet is the problem of exploitation and exploration.
In general during training we always seek balance between exploitation of learned experience and exploring new possibilities.
In the case of Q-learning, this is done  quite artificially most often using by using the $\epsilon$-greedy approach, where with probability $1-\epsilon$ we exploit our knowledge by taking $\arg \max_a$ action according to the $Q$ value.
And with probability $\epsilon$ exploring by taking random action.

By sampling according to the policy $\pi_\theta$ in policy optimization methods this problem is solved more naturally.
Usually, in the begining of the training process when the parameters are initialized, the parametrized probability distribution is close to uniform, which implicitly makes the action selection mechanism to be exploratory.
And as the policy is updated, it becomes more specialized toward the optimal policy, effectively forcing the choice of actions to be more exploitative.
Unfortunately, the exploration described in policy optimization methods is often insufficient, and policies tend to get stuck at bad local optima despite initial uniform distribution.
On that account explorative mechanism in the form of bonus entropy(\cite{EntropyRegularization}) is often being utilized and also suggested by the PPO authors.
By adding a small bonus for policy entropy, the policy is slightly forced toward uniform distribution, shifting toward exploratory behavior.
Hence final PPO objective may have a following form:

\[
  L_t^{CLIP+VF}(\theta) =   \hat{\mathbb{E}}_t \left[L_t^{CLIP}(\theta) - c_1 L_t^{VF}(\theta)  + c_2S[\pi_\theta](s_t)\right],
\]
where $c_2$ is another coefficient and $S$ denotes an entropy bonus.


\section*{Q-Learning and Policy optimization}\label{QPlusPolicy}
So far we have covered possible the simplest division rule among types of reinforcement learning algorithms.
However, there are actually several algorithms that combine both the Q-learning and Policy optimization.
In these algorithms both $Q$ approximator and policy approximator are being learned concurrently.
Just to name a few of the more popular ones: Deep Deterministic Policy Gradient (DDPG, \cite{DDPG}), Twin Delayed DDPG (TD3, \cite{TD3}), Soft Actor-Critic(SAC, \cite{SAC}).
We won't cover these in detail, as they are beyond the scope of our needs.




