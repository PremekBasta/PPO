\chapter{Our work - Contribution}
In this chapter, we propose a novel approach to injecting explicit diversification into agent behavior through diverse population training.
Previously mentioned traditional population-based methods were not effective enough to add the necessary diversification to the population.
We try to use population training in slight different manner with our primary focus set on two important aspects.
Firsty we try to face well known problems connected with multi agent system by changing our perspective towards single-agent settings and transforming to iterative process.
And secondly we try to address the lack of explicit diversification in standard population-based learning methods by pursuing population policy diversification. 

In following sections we attempt to walk through several different aspects of our population building approach.
These parts can be seen as a soft guideline towards diverse population training algorithm which will be presented at the end of this chapter.

\section{Multi agent settings}

\subsection{Single-agent perspective}
As we could have already seen (\ref{MAS}) multi agent systems can be much more intimidating to tackle down.
One of the proposed suggestion was to try to simplify the sitatio by looking at the multi agent environment from the single-agent point of view where the goal is to learn joint policy.
As discussed before, this is often impossible and mostly inpractical.

Nevertheless we try to, in a sense, attempt to utilize similarly motivated approach.
We propose single-agent learning approach where at any given time only one agent is being learned.
Naturally we cannot simply ommitt the second player from the environment as we cannot deny the multi agent essence from the environment.
We rather look at the environment as being instance of single-agent environment where the parnter is embedded and in a sense forms an integral part of the system.

\subsection{Non-stationarity}
We propose population training to be an iterative process where only one agent is being learned and only after its's training is finished he is included into the population.
Once an agent is added into the population it's policy is fixed and no further updates are applied for the rest of population training process.
Only agents used in environment embedding are the ones coming from the already trained population of agents.
This claim this effectively solves the non-stationarity problem we encountered in multi agent settings as from the point of learning agent the behavior of its partners never changes.

There is unfortunately downside to this approach.
While previously the environment had its state transition function purely deterministic, now with the partner embedded into the system it becomes stochastic.
Nonetheless, we hypothesize that this may also be in favor of the final robustness, since by experiencing stochastic state transitions, the agent could theoretically be more robust as a result, since it will have to learn how to behave in a more stochastic exploratory environment.

\subsection{Population exploitance}
We hypothesize that by utilizing iterative process with past agents being fixed it could also help with population exploitance.
When set of agents are trained concurrently together they may all exploit their behavior towards shared experience.
It could probably happen that the new agents might still try to overfit their behavior towards the population agents.
However, by fixing previous past agents we try to break this relation at least from one side of the couple.
We are optimistic that this, in conjunction with further diversification factors, will greatly reduce exploitation of the population.


\section{Population diversification}
A mere change in our perception towards single-agent settings would probably not be enough of an improvement towards agent robustness.
Therefore we propose several ideas to promote population diversification.
\subsection{Inicialization}
To start with, inspired by original motivation behind all population-based methods, we follow the idea that populations should be in general always more diverse than single agent.
Hence when utilized in training this diverse robustness should be passed onto the next learned agent.
In order for this to be true the previous condition on non existence of population exploitance must hold.
Additionaly we already proposed iterative training process where we suppose preexistence of some set of agents utilized for partner embeddeding.

We propose that in order for other diversification factors to work we have to start with some initial population for partner embedding sampling that is already somewhat diverse.
One could argue that if we had a method for creating such initial diverse population we could just stop there and extend this population using such method.
However, we do not see the perfect diversity and robustness of initial population to be crucial. 
We rather expect the inital population to introduce somewhat mildly different set of behaviors just to give a resonable starting point for the rest of population training.

\subsection{KL divergence}
We hypothesize that preexistence of diverse population is just a soft precondition.
We propose that the main ingredient behind our diversity building process lies in attempting to shift the behavior of each consequentially trained agent to be as different as possible from the behaviors in the existing population.
Since sampled population partners are embedded into the environment we can look at this approach as domain randomization technique.


To emphasize the difference between two behaviors, we must first be able to measure it.
There are arguably various possible ways how to express difference between two behaviors including methods discussed in evaluation approaches (\ref{RobustnessEvaluation}).
Nevertheless since the agent behavior is represented by parametrized policy we suppose that one of the more straightforward methods for low-level difference could be to look at the difference between the two policy distributions.

When measuring the distance of the probabilty distribution the Kullback-Leibler divergence (\cite{KLDivergence}) often comes in mind.

\[
    D_{KL}(P||Q) = \sum_{x \in X}P(x)\log\left(\frac{P(x)}{Q(x)}\right)
\]

Although it is not a metric since it is not symmetric in the two distributions and also it does not satify the triangle inequality, we can still utilize it as we are mostly interested in relative distribution differences rather than absolute values.
We propose idea of incorporating maximization of KL divergence as a tool to differentiate new agents from the ones that are already contained in the population.

We suggest that there are two different suitable ways how to incorporate KL divergence maximization into the process.
Firstly, KL divergence between policy that is currently being trained and the policies in the population can be used during episode sampling by interpreting it as another additional partial reward based on a current state of the environment.
And secondly, similar approach can be applied on the level of PPO objective function where another additional objective term based on maximization of KL divergence can be added.

It may seem that these two approaches might have the same effect.
However, we believe that they work in a slightly different manner.
When applied in the PPO objective the term is trying to differentiate current policy no matter the current state of the environment.
While applying the KL divergence bonus as partial reward based on the state might work as a exploration enhancing technique since it should push the agent to visit more the states where it's current policy differs the most which could potentially lead to state trajectories that have not been seen by the agents in the population.   

\subsubsection{KL divergence coefficients}
Applying the absolute value of KL divergence the way it is calculated using the formula would not work in our favor as we have to consider the values range with regard to the environment properties.
KL divergence is non negative but it can easily reach values of infinity when not bounded.
Additionaly we need to consider suitable multiplicative coefficient to scale the values to reasonable values.

When new parametrized policy $p$ is initialized it produces the probability distribution that is close to uniform.
Having sampled several already trained self-play agents and evaluated in same initial states we obatined specialized policies yielding the probability distributions $q_1, q_2, q_3$.


\begin{table}[htbp]
    \small
    \centering
    \begin{tabular}{lll}
      \toprule
      \                                & Probability distribution     & $D_{KL}(p||q)$         \\ \midrule
      \textit{p}     &                                   &                                   \\ \midrule            
      $q_1$                     & $0.99$                            & $0.98$                            \\
      $q_2$                         & $0.98$                            & $0.95$                            \\
      $q_3$                                & $10^{-3}$                         & $4 \cdot 10^{-4}$                               \\
      
     \bottomrule
    \end{tabular}
    \caption{Hyperparameters for self-play agent training on Cramped room layout}
    \label{tab:KLDiv-coefiicents}
  \end{table}




\begin{figure}[!ht]
    \centering
    \includegraphics*[width=14cm]{../img/Forced_coordination_CNN_POP_NO_DIF(2).png}

    \caption{CNN Population with no diversification cross play evaluation}
    \label{CNNPopNoDiffBestInitFixation}
    \medskip
    \small 
    5 populations trained using different seeds with no additional diversification cross-play evaluated against Self-play evaluation set

\end{figure}

\subsubsection{Initial experiments visualised}

After initial experiment (figure \ref{CNNPopNoDiffBestInitFixation}) where population was trained we can see that first three self-play agents used for initialization of population play important role.
In three seeds (10,11,13) initial population strongly represents one set of behaviors and lacks enough diversity.
As a result as agent training progresses during population building, agents learn to fixate solely on this kind of behavior as it promises maximal available outcome and ignore other types of behaviors.

To confront this issue we propose two suggestion.
Firstly, in this initial experiments first three trained self-play agents were selected for population inicialization automatically.
However, it can happen, as can be seen mostly in experiments performed on seed values 10 and 11, that several of agents can actually represent same type of behavior thus forcing the rest of the population to be strongly biased towards it.
We instead propose that first five self-play agents will be trained and then evaluated with the evaluation set.
And finally out of these five three will be selected such three agents that these are both successful and also pair-wise different as much as possible.

And secondly, during training all available agents from current population are present at each training epoch as they are equally distributed among 30 parallel environments.
This guarantes that the dominant type of behavior from population initialization is always represented during training, which enables newly trained agent to move towards this kind of behavior.
Instead we propose not to sample all agents from current population, but rather sample only few of them to give chance to ommitt the dominant and allow trained agent to focus on the remaining types.
After performing simplifed experiment we concluded with suitable value three for number of sampled partners.  


\begin{figure}[!ht]
    \centering
    \includegraphics*[width=14cm]{../img/SimpleCnnExperimentsAvg.png}

    \caption{Average cummulative reward of final agents}
    \label{AvgCummulativeRewardEvaluated}
    \medskip
    \small 
    Average cummulative reward of the two final agents trained in last phase of the population building.
    Includes values also for cross-play evaluation of self-play evaluation set for reference. 

\end{figure}

\subsubsection{Average cummulative reward evaluation}

As discussed in previous chapters (\ref{RobustnessEvaluation}) performing evaluation of acquired population using usual methods is not very intuitive. 
When standard average cummulative reward metric is used (figure \ref{AvgCummulativeRewardEvaluated}) there are no apparent improvements and results vary a lot which we believe could be attributed to two factors.
Firstly, as we just discussed, the average cummulative rewards of initial populations vary also a lot when evaluated against self-play evaluation set.
And secondly, even when final population agents are trained there is no guarante, that will not end up in some suboptimal local optima with respect to current population training.


\subsubsection{Ordered average cummulative reward evaluation}
When agents are ordered by the evaluation with other agents it shows significant improvement in terms of early mild cooperation (\ref{SimpleCNNOrderedAvg}). 
Results are even more amplified when not looking at average case, but rather at 0.15 quantile (figure \ref{SimpleCNNOrderedAvg0.15Q}).
{\color{blue} This might be interpreted as looking at worse case cooperation? In a sense that our final agent is able to cooperate to extent of cca 80 shared rewards with way wider set of evaluation agents? }

Sometimes even in the middle of population there can be already agents that behave nicely (\ref{FSTuple4thOrderedAvg})

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/SimpleCNNOrderedAvg.png}

    \caption{Average cummulative reward of final agents}
    \label{SimpleCNNOrderedAvg}
    \medskip
    \small 
    Average cummulative reward of the two final agents trained in last phase of the population building.
    Includes values also for cross-play evaluation of self-play evaluation set for reference. 
    Average, 0.25Q, 0.75Q

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/SimpleCNNOrderedAvg0.15Q.png}

    \caption{Average cummulative reward of final agents, 0.15quantile}
    \label{SimpleCNNOrderedAvg0.15Q}
    \medskip
    \small 
    Average cummulative reward of the two final agents trained in last phase of the population building.
    Includes values also for cross-play evaluation of self-play evaluation set for reference. 
    0.15Q, Min, 0.3Q

\end{figure}


\subsubsection{Frame stacking experiments}
TODO: two variants

Looks like temporal feature made even basic self-play agent a bit more cooperative (figure \ref{FSVariantsOrderedAvg}).

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FSVariantsOrderedAvg.png}

    \caption{Average cummulative reward of final agents}
    \label{FSVariantsOrderedAvg}
    \medskip
    \small 
    Average cummulative reward of all agents from evaluation set.
    All three types of self-play agent designs included.

    Average, 0.25Q, 0.75Q

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FSTuple4thOrderedAvg.png}

    \caption{Average cummulative reward of 4th agent within the population part}
    \label{FSTuple4thOrderedAvg}
    \medskip
    \small 
    Average cummulative reward of 4th agent from population.
    Using tuple frame stacking mechanism.
    Sometime the final pop agent is not the best one.

    Average, 0.25Q, 0.75Q

\end{figure}


\subsection{Populations evaluation}
In here we will not evaluate agents against self-play agents, but evaluate final population agents against each other among different experiments settings.
(Simple CNN \ref{FinalPopFinalPopSimpleCnnAvg}). (FS Channels \ref{FinalPopFinalPopFsChannelsAvg}). (FS Tuple \ref{FinalPopFinalPopFsTupleAvg})
\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FinalPopFinalPopSimpleCnnAvg.png}

    \caption{Evaluation of final population agent using simple CNN}
    \label{FinalPopFinalPopSimpleCnnAvg}
    \medskip
    \small 

    Average, 0.25Q, 0.75Q

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FinalPopFinalPopFsChannelsAvg.png}

    \caption{Evaluation of final population agent using channels frame stacking}
    \label{FinalPopFinalPopFsChannelsAvg}
    \medskip
    \small 

    Average, 0.25Q, 0.75Q

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FinalPopFinalPopFsTupleAvg.png}

    \caption{Evaluation of final population agent using tuple frame stacking}
    \label{FinalPopFinalPopFsTupleAvg}
    \medskip
    \small 

    Average, 0.25Q, 0.75Q

\end{figure}




% \section{Idea}


% We will transform population learning into an incremental process, where only newly added agents are learned.
% This process is designed to be quite general with respect to the potentially pre-existing set of autonomous agents.

% The building process will start with a population of whatever pre-trained agents are available, no matter what the source of agents is.
% Consequently, a new agent is learned with respect to the previous population by using some diversification techniques.
% After the agent is trained to cooperate with individual agents within the existing population while maintaining its diversity, it is added to the fixed population and the next agent can be trained.

% By fixing the previously trained agent, we effectively solve the problem of non-stationarity, since no two agent policies are changed at the same time.
% By embedding sampled partners from the population into the environment, we can consider the environment as a single agent from the point of view of the agent being trained.
% However, despite the fact that the original overcooked environment is deterministic from the point of view of the transition function, by embedding partners whose policies will be mostly stochastic into the environment, the transition function becomes stochastic from the point of view of the trained agent.
% The same applies to the reward function.
% Since part of our proposed diversification methods involve reward augmentation based on the agent's policy, the environment's reward function will no longer be stationary throughout training due to the learning policy.

% {\color{blue} Po prostudovani a sepsani MARL kapitoly mi pripada, ze muj pristup popsany vyse je vlastne hrozne slaby, obzvlaste tim, ze trenuju ciste reaktivni agenty bez jakekoliv pameti (uvazuji jen soucasnou CNN reprezentaci stavu, bez RNN, bez predeslych akci partnera, bez historie predchozich stavu),
% tim vlastne ani nejsem schopny komplexne zachytit nejakou high-level strategii partnera, namisto toho se proste ucim jak se nejlepe (robustne?) zachovat v jednom konkretnim stavu vzhledem k potencialne vice moznych strategii, kterymi by se mohl partner v danem stavu ridit.
% Tim, ze CNN kompletne popisuje stav prostredi, tak ani nemam partial observability a vstup kritika jsem nijak nerozsiroval.
% Proste pripada mi, ze jsem v tom reseni nezahrnul moc zadne techniky popsane pro MAS.

% Pripada mi ze i tak ty experimenty ktere zkousim a navrhuju jsou zajimave, jen mi pripadaji takove vytrzene z toho MAS kontextu - ale o tom jsme si myslim minule bavili ze nevadi.
% }


% Diverse population can be thought of as domain randomization technique
% "Given just this, it is unclear what the agent should do: the optimal
% policy for the agent depends heavily on the human’s policy, which the agent has no control over"

% "From the perspective of game theory, we are interested
% in n-person games in which the players have a shared or joint
% utility function. In other words, any outcome of the game has
% equal value for all players. Assuming the game is fully co-
% operative in this sense, many of the interesting problems in
% cooperative game theory (such as coalition formation and ne-
% gotiation) disappear. Rather it becomes more like a standard
% (one-player) decision problem, where the collection of n play-
% ers can be viewed as a single player trying to optimize its be-
% havior against nature." 

% "Solutions to the coordination problem can be divided into
% three general classes, those based on communication, those
% based on convention and those based on learning"

% Convention probably does not make sense as we have ad hoc partner
% Craig Boutilie 1996

% \section{Our definition(s?) of robustness}
% Probably just average of pair results (non diagonal in case of same sets).
% Maybe percentage of pairs who surpassed some threshold reward?

% \section{Population construction}

% \subsection{SP agents initialization}
% \textbf{One agent is not enough?}

% \subsection{population partner sampling during training}
% \textbf{See if playing with whole population at once differs from one random partner for episode}

% \subsection{Final agent training}

% \section{Diverzification}
% \textbf{maximize kl divergence among population partners policies}

% \subsection{Population policies difference rewards augmentation}
 
% \subsection{Population policies difference loss}
