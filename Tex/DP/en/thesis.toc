\contentsline {chapter}{Introduction}{3}{chapter*.2}%
\contentsline {chapter}{\numberline {1}Introduction to reinforcement learning}{5}{chapter.1}%
\contentsline {section}{\numberline {1.1}Markov decision process}{5}{section.1.1}%
\contentsline {section}{\numberline {1.2}Single-agent environment}{5}{section.1.2}%
\contentsline {chapter}{\numberline {2}Reinforcement learning algorithms}{10}{chapter.2}%
\contentsline {section}{\numberline {2.1}Q-Learning}{10}{section.2.1}%
\contentsline {section}{\numberline {2.2}Policy gradient methods}{13}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Idea}{13}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Policy Gradient Theorem}{13}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Vanilla Policy Gradient}{14}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Trust Region Policy Optimization}{15}{subsection.2.2.4}%
\contentsline {subsection}{\numberline {2.2.5}Proximal Policy Optimization}{17}{subsection.2.2.5}%
\contentsline {chapter}{\numberline {3}Multi-agent environment}{21}{chapter.3}%
\contentsline {section}{\numberline {3.1}Multi-agent Markov Decision Process}{21}{section.3.1}%
\contentsline {section}{\numberline {3.2}Decentralized Partially Observable MDP}{21}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Nash Equilibria}{23}{subsection.3.2.1}%
\contentsline {section}{\numberline {3.3}Learning schemes}{23}{section.3.3}%
\contentsline {section}{\numberline {3.4}Non-stationarity}{24}{section.3.4}%
\contentsline {section}{\numberline {3.5}RL algorithms}{25}{section.3.5}%
\contentsline {chapter}{\numberline {4}Overcooked environment}{27}{chapter.4}%
\contentsline {section}{\numberline {4.1}Overcooked game}{27}{section.4.1}%
\contentsline {section}{\numberline {4.2}Basic layouts}{27}{section.4.2}%
\contentsline {section}{\numberline {4.3}Environment description}{29}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Action space}{29}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}State representation}{29}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {4.3.3}Rewards}{30}{subsection.4.3.3}%
\contentsline {subsection}{\numberline {4.3.4}Initial state}{30}{subsection.4.3.4}%
\contentsline {chapter}{\numberline {5}Related work}{32}{chapter.5}%
\contentsline {section}{\numberline {5.1}Human cooperation}{32}{section.5.1}%
\contentsline {section}{\numberline {5.2}AI-AI cooperation}{32}{section.5.2}%
\contentsline {section}{\numberline {5.3}Problem of robustness}{34}{section.5.3}%
\contentsline {chapter}{\numberline {6}Our work - Preparation}{36}{chapter.6}%
\contentsline {section}{\numberline {6.1}Framework}{36}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}Modifications}{36}{subsection.6.1.1}%
\contentsline {section}{\numberline {6.2}Self-play}{38}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Random state initialization}{39}{subsection.6.2.1}%
\contentsline {subsection}{\numberline {6.2.2}Focus on Forced coordination}{41}{subsection.6.2.2}%
\contentsline {chapter}{\numberline {7}Our work - Contribution}{42}{chapter.7}%
\contentsline {section}{\numberline {7.1}Idea}{42}{section.7.1}%
\contentsline {section}{\numberline {7.2}Our definition(s?) of robustness}{43}{section.7.2}%
\contentsline {section}{\numberline {7.3}Population construction}{43}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}SP agents initialization}{43}{subsection.7.3.1}%
\contentsline {subsection}{\numberline {7.3.2}population partner sampling during training}{43}{subsection.7.3.2}%
\contentsline {subsection}{\numberline {7.3.3}Final agent training}{43}{subsection.7.3.3}%
\contentsline {section}{\numberline {7.4}Diverzification}{43}{section.7.4}%
\contentsline {subsection}{\numberline {7.4.1}Population policies difference rewards augmentation}{43}{subsection.7.4.1}%
\contentsline {subsection}{\numberline {7.4.2}Population policies difference loss}{43}{subsection.7.4.2}%
\contentsline {chapter}{Conclusion}{44}{chapter*.44}%
\contentsline {chapter}{Bibliography}{45}{chapter*.45}%
