\contentsline {chapter}{Introduction}{3}{chapter*.2}%
\contentsline {chapter}{\numberline {1}Introduction to reinforcement learning}{4}{chapter.1}%
\contentsline {section}{\numberline {1.1}Markov decision process}{4}{section.1.1}%
\contentsline {chapter}{\numberline {2}RL algorithms}{8}{chapter.2}%
\contentsline {section}{\numberline {2.1}Q-Learning}{8}{section.2.1}%
\contentsline {section}{\numberline {2.2}Policy gradient methods}{11}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Idea}{11}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Policy Gradient Theorem}{11}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Vanilla Policy Gradient}{12}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Trust Region Policy Optimization}{13}{subsection.2.2.4}%
\contentsline {subsection}{\numberline {2.2.5}Proximal Policy Optimization}{15}{subsection.2.2.5}%
\contentsline {chapter}{\numberline {3}Multi agent environments for RL ???}{19}{chapter.3}%
\contentsline {section}{\numberline {3.1}Multi-agent Markov Decision Process}{19}{section.3.1}%
\contentsline {section}{\numberline {3.2}Decentralized Partially Observable MDP}{19}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Nash Equilibria}{21}{subsection.3.2.1}%
\contentsline {section}{\numberline {3.3}Learning schemes}{21}{section.3.3}%
\contentsline {section}{\numberline {3.4}Non-stationarity}{22}{section.3.4}%
\contentsline {section}{\numberline {3.5}RL algorithms}{22}{section.3.5}%
\contentsline {chapter}{\numberline {4}Overcooked environment}{24}{chapter.4}%
\contentsline {section}{\numberline {4.1}Overcooked game}{24}{section.4.1}%
\contentsline {section}{\numberline {4.2}Basic layouts}{24}{section.4.2}%
\contentsline {section}{\numberline {4.3}Environment description}{25}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Actions space, episode horizont, shaped rewards, state representation MLP vs CNN}{25}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Reset state static position, index switching, Randomization function}{25}{subsection.4.3.2}%
\contentsline {chapter}{\numberline {5}Related work}{26}{chapter.5}%
\contentsline {section}{\numberline {5.1}Human-ai cooperation results}{26}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Human cooperation}{26}{subsection.5.1.1}%
\contentsline {section}{\numberline {5.2}Problem of robustness}{26}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Problem of robustness definition}{26}{subsection.5.2.1}%
\contentsline {section}{\numberline {5.3}AI-AI coordination}{26}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Aproaches}{26}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}Results}{26}{subsection.5.3.2}%
\contentsline {chapter}{\numberline {6}Our work - Preparation}{27}{chapter.6}%
\contentsline {section}{\numberline {6.1}Utilized framework}{27}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}Modifications of stable baselines}{27}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}NN structure modification}{27}{subsection.6.1.2}%
\contentsline {subsection}{\numberline {6.1.3}Hyperparameters random search}{27}{subsection.6.1.3}%
\contentsline {subsection}{\numberline {6.1.4}Randomization function correction}{27}{subsection.6.1.4}%
\contentsline {section}{\numberline {6.2}Self-play}{27}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Training}{27}{subsection.6.2.1}%
\contentsline {subsection}{\numberline {6.2.2}Results}{27}{subsection.6.2.2}%
\contentsline {chapter}{\numberline {7}Our work - Contribution}{28}{chapter.7}%
\contentsline {section}{\numberline {7.1}Our definition(s?) of robustness}{28}{section.7.1}%
\contentsline {section}{\numberline {7.2}Population construction}{28}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}SP agents initialization}{28}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}population partner sampling during training}{28}{subsection.7.2.2}%
\contentsline {subsection}{\numberline {7.2.3}Final agent training}{28}{subsection.7.2.3}%
\contentsline {section}{\numberline {7.3}Diverzification}{28}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}Population policies difference rewards augmentation}{28}{subsection.7.3.1}%
\contentsline {subsection}{\numberline {7.3.2}Population policies difference loss}{28}{subsection.7.3.2}%
\contentsline {chapter}{Conclusion}{29}{chapter*.25}%
\contentsline {chapter}{Bibliography}{30}{chapter*.26}%
