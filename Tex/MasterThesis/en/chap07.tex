\chapter{Diverse population building}\label{DiversePopulationMethodChapter}
In this chapter, we propose a novel approach to introduce explicit diversification into agent behavior by training diverse populations.
The traditional population-based methods mentioned above are not effective enough to add the necessary diversification to the population.
We try to use population training in a slightly different way, focusing on two important aspects.
First, we try to address well-known problems associated with multi-agent systems by changing our perspective to single-agent settings and transforming to an iterative process.
Secondly, we try to address the lack of explicit diversification in standard population-based learning methods by pursuing population policy diversification.

In the following sections, we describe several different aspects of our population building approach.

The first series of experiments will be conducted on a pre-selected Forced Coordination layout, where we will gather important observations before evaluating our final approaches on two additional layouts.

The latest source code of our solution and discussed experiments can be found in the following repository:
\url{https://github.com/PremekBasta/PPO}


\section{Multi-agent setting simplification}

\subsection{Single-agent perspective}
As we have already seen (Chapter \ref{MAS}), multi-agent reinforcement learning can be much more intimidating to tackle.
One of the suggestions was to try to simplify the situation by looking at the multi-agent environment from a single-agent point of view, where the goal is to learn a common policy.
As discussed above, this is often impossible and mostly impractical.

Nevertheless, in a sense, we try to use a similarly motivated approach.
We propose a single-agent learning approach, where only one agent is trained at a time.
Of course, we cannot simply ommit the other agent from the environment because we cannot deny the multi-agent nature of the environment.
Rather, we consider the environment as an instance of a single-agent environment in which the partner is embedded and in some sense forms an integral part of the system.

\subsection{Non-stationarity}
We propose population training as an iterative process where only one agent is trained and only after its training is complete it is added to the population.
Once an agent is added to the population, its policy is fixed and no further updates are applied for the rest of the population training process.
Only agents from the already trained population are used in the environment embedding.
We claim that this effectively solves the non-stationarity problem we encountered in multi-agent settings, since from the point of agent learning, the behavior of its partners never changes.

Unfortunately, there is a downside to this approach.
Whereas before the environment had a purely deterministic state transition function, now with the system embedded partner whose actions are stochastically sampled, the transition function becomes stochastic.
However, we hypothesize that this may also be beneficial for the final robustness, since by experiencing stochastic state transitions, the agent could theoretically be more robust as a result, since it will have to learn how to behave in a more stochastic exploratory environment.
Given that the sampled population partners are embedded in the environment, we can think of this approach as a domain randomization technique.

\subsection{Population exploitation}
We hypothesize that by using an iterative process where past agents are fixed, it could also help with population exploitation (recall Section \ref{PopulationExploitance}).
If a set of agents is trained together at the same time, they can all exploit their behavior towards the shared experience.
It is likely that the new agents will still try to overfit their behavior to the population agents.
However, by fixing the previous past agents, we try to break this relationship from at least one side of the pair.
We are optimistic that this, combined with other diversification factors, will greatly reduce exploitation of the population.


\section{Population building}
A mere change in our perception of single-agent settings would not need to be enough to improve agent robustness.
Therefore, we propose several ideas to promote population diversification.
\subsection{Initialization}
First, inspired by the original motivation behind all population-based methods, we follow the idea that populations in general should always be more diverse than a single agent.
Thus, when utilized in training, this diverse robustness should be passed on to the next learned agent.
In order for this to be true, the previous condition about the non-existence of population exploitation must hold.
Additionally, we have already proposed an iterative training process where we assume the pre-existence of a set of agents used for partner embedding.

We propose that in order for other diversification factors to work, we need to start with an initial population for partner embedding sampling that is already somewhat diverse.
One could argue that if we had a method for creating such a diverse initial population, we could just stop there and expand that population using such a method.
However, we hope that perfect diversity of the initial population should not be critical. 
Rather, we expect the initial population to introduce a slightly different set of behaviors just to provide a reasonable starting point for the rest of the population training.

\subsection{KL divergence}
We hypothesize that the pre-existence of a diverse population is only a soft constraint, and we hope that the main ingredient behind successful diversity building lies in the attempt to shift the behavior of each successively trained agent to be as different as possible from the behaviors in the existing population.



To emphasize the difference between two behaviors, we must first be able to measure it.
There are arguably several possible ways to express the difference between two behaviors, including methods discussed in evaluation approaches (Section \ref{RobustnessEvaluation}).
However, since the agent's behavior is represented by a parameterized policy, we suppose that one of the more straightforward methods for low-level difference might be to look at the difference between the two policy distributions.

The Kullback-Leibler divergence (\cite{KLDivergence}) often comes to mind when measuring the distance between probability distributions.

\[
    D_{KL}(P||Q) = \sum_{x \in X}P(x)\log\left(\frac{P(x)}{Q(x)}\right)
\]

Although it is not a metric since it is not symmetric in the two distributions and it does not satisfy the triangle inequality, we can still utilize it since we mostly are interested in relative distribution differences rather than absolute values.
We propose the idea of incorporating the maximization of the KL divergence as a tool to differentiate new agents from those already contained in the population by taking the average of the KL divergence of the trained policy and the policies in the population.

We propose that there are two different appropriate ways to incorporate KL divergence maximization into the process.
First, KL divergence between the currently trained policy and the policies in the population can be used during episode sampling by interpreting it as another additional partial reward based on the current state of the environment.
And second, a similar approach can be applied at the level of the PPO objective function, where another additional term based on maximizing the KL divergence can be added.

It may seem that these two approaches have the same effect.
However, we believe they work in slightly different ways.
When applied in the PPO objective, the term attempts to differentiate current policy regardless of the current state of the environment.
While applying the KL divergence bonus as a partial reward based on state might work as an exploration-enhancing technique since it should push the agent to visit more often the states where its current policy differs the most, potentially leading to state trajectories that agents in the population have not seen.

\subsubsection{KL divergence coefficients}
Applying the value of the KL divergence, as calculated by the formula, would not work in our favor, since we have to consider the value ranges with respect to the environmental properties.
If not bounded, the KL divergence term could dominate the objective function.
Additionally, we need to consider a suitable multiplicative coefficient to scale the values to a reasonable range.

When a new parameterized policy $p$ is initialized, it produces the probability distribution that is close to uniform.
After sampling several already trained self-play agents and evaluating them in the same initial states, we obtained specialized policies yielding the probability distributions $q_1, q_2, q_3$ (see Table \ref{tab:KLDiv-distributions}).


\begin{table}[htbp]
    \small
    \centering
    \begin{tabular}{lll}
      \toprule
      \                                & Probability distribution     & $D_{KL}(p||q)$         \\ \midrule
      \textit{p}     &$[0.1679, 0.1647, 0.1644, 0.1663, 0.1713, 0.1654]$                                   &                                   \\ \midrule            
      $q_1$                     & $[0.0336, 0.0729, 0.0467, 0.5419, 0.1431, 0.1617]$                            & $0.449$                            \\
      $q_2$                         & $[0.0453, 0.1906, 0.1270, 0.2221, 0.0942, 0.3207]$                            & $0.183$                            \\
      $q_3$                                & $[0.1875, 0.1851, 0.1663, 0.1369, 0.1414, 0.1828]$                         & $0.009$                               \\
      
     \bottomrule
    \end{tabular}
    \caption{KL divergence values ilustrated on trained policies on same random inital state}
    \label{tab:KLDiv-distributions}
\end{table}

These values of KL divergence can give us an idea of the absolute values when new policies are compared to already trained agents.
We believe that the probability distributions $p$ and $q_1$ are different enough in terms of what we are trying to achieve when training a population of diverse agents.
By considering the absolute value of the KL divergence of these two distributions, we try to derive appropriate KL divergence coefficients for loss and reward augmentation.

We hypothesize that the KL divergence term, which we add to the PPO objective, should have comparable weight to the entropy bonus (recall Section \ref{PPO}), as this bonus is also designed to be a mild exploratory pressure rather than a main objective.
Entropy in the early stages of training corresponds to a uniform probability distribution, yielding in our case absolute values of $1.8$. 
This combined with the initial entropy bonus coefficient of $0.1$ (hyperparameter Table \ref{tab:hyperparameters-algo}) yields absolute values of cca $0.18$.
In the final stage, when the policy is already trained, the entropy usually decreases to absolute values of about $1.2$, which combined with the entropy bonus end coefficient of $0.03$ produces a final value of $0.036$.

As far as reward augmentation is concerned, we believe that when all KL divergence bonuses are summed over the 400 steps of the episode, they should correspond to only a fraction of what can be obtained by following the main objective of soup delivery.
We propose to design reward clipping values in equivalence to the number of soups delivered.
We design three reward augmentation experiments (R$0$, R$1$, R$2$) which, when the maximum clipped reward is summed over the horizon of $400$ steps, correspond to $0.5$, $1$, and $1.5$ soups delivered.

After performing some trivial experiments, we came up with the following experiment settings (Table \ref{tab:KLDiv-coefiicents}), which we want to evaluate in our following experiments.



\begin{table}[htbp]
    \small
    \centering
    \begin{tabular}{lllll}
      \toprule
      Experiment Name        & Reward coefficent     & Reward clip  & Loss coefficent  & Loss clip          \\ \midrule

      No divers.     & $0.0$     & $0.0$   & $0.0$     & $0.0$                        \\\midrule  
      R$0$     & $0.08$     & $0.025$   & $0$     & $0$                        \\
      R$1$     & $0.15$     & $0.05$   & $0$     & $0$                        \\
      R$2$     & $0.1$     & $0.075$   & $0$     & $0$                        \\\midrule
      L$0$     & $0$     & $0$   & $0.08$     & $0.03$                        \\
      L$1$     & $0$     & $0$   & $0.12$     & $0.07$                        \\      
      L$2$     & $0$     & $0$   & $0.1$     & $0.15$                        \\\midrule      
      R$0$L$0$     & $0.08$     & $0.02$   & $0.08$     & $0.02$                        \\
      R$1$L$1$     & $0.1$     & $0.04$   & $0.1$     & $0.03$                        \\
      
     \bottomrule
    \end{tabular}
    \caption{Summary of experiments utilizing KL divergence}
    \label{tab:KLDiv-coefiicents}
\end{table}


\subsection{Population structure}
Finally, we need to provide a description of the entire proposed population building process.
We have already dealt with population initialization, where we proposed how the initial agents could be constructed, and we also designed diversification tools to enhance the behavioral diversity of the population.

However, our ultimate goal is to provide a procedure that produces a final agent or set of final agents that should be robust and highly cooperative.
By applying diversification techniques, we effectively train agents that do not fully optimize towards the main objective, but their performance may be degraded due to diversification.

After training several diversified population agents, we propose to train final agents by exposing them to the already built population, but without further diversification pressure.
We propose to train the first final agent by exposing it to a population that includes also the agents that were part of the initialization and then train another agent with the same population, but without the initial agents.

To ensure that the final agents have enough space to learn how to cooperate with the entire population, we increase the number of training time steps by a factor of $1.5$.

Unless otherwise stated, in all of the following experiments we will evaluate our trained agents against the reference set of 30 self-play agents that are of the same type as our population-trained agents.

\section{Simple convolution experiments}

\subsubsection{Initial experiments visualised}

After the initial population training experiment (Figure \ref{CNNPopNoDiffBestInitFixation}), where we cross-evaluated the trained population agents against the reference set, we can see that the first three self-play agents used to initialize the population play an important role.
In three seeds (10,11,13) the initial population seems to represent strongly the same one set of behaviors and lacks diversity towards other behaviors.
It seems that as population training progresses, agents learn to fixate only on this type of behavior, as it promises maximum available outcome, and ignores other types of behaviors.

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=14.5cm]{../img/Forced_coordination_CNN_POP_NO_DIF(2).png}

    \caption{Diversity free populations cross-play evaluation}
    \label{CNNPopNoDiffBestInitFixation}
    \medskip
    \small 
    Cross play evaluation of five populations trained with no diversification pressures against self-play agents

\end{figure}

To address this issue, we propose two suggestions.
First, in these initial experiments, the first three trained self-play agents were automatically selected for population initialization.
Instead, we propose to perform the initial population agent selection manually by first training five self-play agents, evaluating them with any kind of evaluation method (in our case evaluating them with another set of self-play agents), and finally selecting from these five such three agents that are both successful and also pairwise different as much as possible.

And second, during training, all available agents from the current population were present at each training epoch, as they were equally distributed across 30 parallel environments.
This guarantees that the dominant type of behavior from the population initialization was always present during training, which allowed newly trained agents to move towards this type of behavior.
Alternatively, we propose not to sample all agents from the current population, but to sample only a few to give a chance to omit the dominant ones and allow the trained agent to focus also on the remaining types.
After performing a simplified experiment, we decided that a suitable value seems to be three for the number of sampled partners. 


\begin{figure}[!ht]
    \centering
    \includegraphics*[width=14cm]{../img/SimpleCnnExperimentsAvg.png}

    \caption{Average cummulative reward of final agents}
    \label{AvgCummulativeRewardEvaluated}
    \medskip
    \small 
    Average cummulative reward of the two final agents trained in last phase of the population building.
    Includes values also for cross-play evaluation of self-play evaluation set for reference. 

\end{figure}

\subsubsection{Average outcome evaluation}

As discussed in previous chapter (Section \ref{RobustnessEvaluation}), it is not very intuitive to perform the evaluation of the obtained population using the usual methods. 
When the overall cross-play average outcome metric is used (Figure \ref{AvgCummulativeRewardEvaluated}), there are only slight improvements over self-play agents, but the results vary widely, which we believe is due to two factors.
First, as we just discussed, the overall cross-play average score of agents in the population initializations also varies a lot between different seeds.
And second, even when the final population agents are trained, there is no guarantee that they will not end up in some suboptimal local optima with respect to the current population.


\subsubsection{Ordered average outcome evaluation}\label{OrderedEvaluation}
We propose to use a different metric to evaluate the population of agents.
After performing the cross-play evaluation against reference set, we suggest to order the results in ascending order.
By using this ordering, we lose the information about the variety of specific pairwise cooperation in terms of whether different agents managed to cooperate with different set of evaluation agents.
On the other hand, this provides us a percentil of how many evaluation agents our evaluated agents are able to cooperate with at a specific cooperation level.
We define a level of cooperation as a fixed selected value of average cumulative rewards.
For this evaluation metric, the best of the two final population agents is used.

Using this evaluation metric and averaging over the results of three different seeds, we can see an evident difference in the cooperation curve (see Figure \ref{SimpleCNNOrderedAvg}).
Looking at the cooperation level with an outcome value of 20, we can see that all agents from all different experiment settings managed to cooperate with a much larger set of evaluation self-play agents.
Similar results can be seen at the cooperation level with a value of about 120.
The most important change in the curve progression can be seen in the last part of the ordered results, where the self-play agents start to catch up and overcome the results of the diversified population agents.
We believe that this result is expected, since the final part of the results corresponds to the pairwise result of self-play agents with similar learned strategy, which should lead to a strong cooperative result.

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=11cm]{../img/SimpleCNNOrderedAvg.png}

    \caption{Forced coordination: Simple CNN, Average, ordered averaged cross-play of population final agents vs self-play agents}
    \label{SimpleCNNOrderedAvg}
    \medskip
    \small 
    Average of ordered results of cross-play evaluation of best population final agents against self-play agents.
    Includes values also for cross-play evaluation of self-play evaluation agent set for reference, where same agent pairs are ommited.
    (Average, 0.25Q, 0.75Q)

\end{figure}

Probably more interesting result can be seen if we apply the same metric, but instead of considering the average result, we focus on the $0.15$ quantile value.
By lowering the quantile, we are looking at the worse case cooperation results obtained on different seeds.
Looking at the visualized results (Figure \ref{SimpleCNNOrderedAvg0.15Q}), we can see that in the worse case there were far fewer self-play agents that managed to cooperate with other agents at a cooperation level value of at least 80.
This result can be seen as an argument for the cooperative capabilities of our approach, since it shows greater robustness with other agents at lower, but reasonable, cooperation levels.



\begin{figure}[!ht]
    \centering
    \includegraphics*[width=11cm]{../img/SimpleCNNOrderedAvg0.15Q.png}

    \caption{Forced coordination: Simple CNN, 0.15 quantile, ordered averaged cross-play of population final agents vs self-play agents}
    \label{SimpleCNNOrderedAvg0.15Q}
    \medskip
    \small 
    0.15 quantile of ordered results of cross-play evaluation of best population final agents against self-play agents.
    Includes values also for cross-play evaluation of self-play evaluation agent set for reference, where same agent pairs are ommited.
    (0.15Q, Min, 0.3Q)

\end{figure}

\newpage


\section{Frame stacking}
After performing initial experiments and observing visualized pair-wise results with a set of self-play agents, we often witnessed final agent results that were both unable to cooperate well with the dominant behavior of population initialization agents, but also unable to learn how to cooperate with the remaining behavior types.
As a result, we questioned whether the potential capabilities of our agent architecture were not limited.

So far, we have tried to expose our new agent to various potentially different types of behaviors represented by the previous population agents, in an attempt to teach it to be capable of cooperating with each of them.
In other words, we are trying to learn a policy that responds best to all previous policies.

However, we claim that this might not be possible if the agent has to decide its policy based on a single current state of the environment only.
To illustrate a simple counterexample, we can think of any kind of situation where agents get stuck by repeating the same actions, resulting in the same state as before.
By considering only such a particular state, we cannot be certain whether agents are stuck since we do not know what action our partner will take.
However, if we expect our partner to do an action that, when combined with our action, will lead to a non-stuck state, while our agent may expect us to try to do the same, then we are effectively stuck.

Similarly, we believe that our partner's understanding of the current high-level goal can be significantly improved if several previous steps are available.

To solve this problem, we propose that using some kind of temporal features about previous states is necessary.
We try to propose two slightly different modifications incorporating temporal features that we will try to use in the rest of our experiments.
While we could arguably wish to preserve the entire history of our partner's behavior, we believe that this leads to a significantly more difficult problem where we are trying to model our partner at a much higher level of complexity.
We seek to find a middle ground by considering only a limited history.
Inspired by similar approach applied in successful Atari AI project (\cite{Atari}), we assume that considering the last four states for temporal features should be sufficient for our goal.


\subsection{Channels}
The first of our two approaches is based on extending the global lossless state representation (see Section \ref{StateRepresentation}).
The 22 channels in this representation can generally be divided into three groups.
The first group consists of constant values describing the physical properties of the given kitchen layout. 
This includes layout terrain, source locations of onions and dishes, and also pot and dispenser locations.
The next group consists of locations of currently existing items such as onions, dishes and soups.
And lastly, state consists of mask representing player's location and orientation.

We introduce the idea of extending the current state representation by appending only the player location and orientation masks from the previous three states.
Appending the static layout information channels from the previous states does not add any new information, as they are already present in the current state representation.
Furthermore, we also omit the information about the current instances of objects from the previous states, since these can be inferred in terms of important temporal features by combining their current state, which is present in the current state representation, with the information about the past movements of the agents.

Since there are five channels describing an agent's position and orientation, by including information about the last three previous states, we arrive at our final state representation consisting of 52 channels.

While we are aware that this state representation construction does not follow the same pattern as the original representation, since in the original representation there were no temporal relations between different channels, we are hopeful that the network will be able to develop such a representation on its own.

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FSChannelsOrderedAvg.png}

    \caption{Forced coordination: FS Channels, Average, ordered averaged cross-play of population final agents vs self-play agents}
    \label{FSChannelsOrderedAvg}
    \medskip
    \small 
    Average cummulative reward of the best final agent using channels frame stacking extension for temporal features.    
    (Average, 0.25Q, 0.75Q)

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FSChannelsOrderedQ15.png}
    
    \caption{Forced coordination: FS Channels, 0.15 quantile, ordered averaged cross-play of population final agents vs self-play agents}
    \label{FSChannelsOrderedQ15}
    \medskip
    \small 
    Average cummulative reward of the best final agent using channels frame stacking extension for temporal features. 
    (0.15Q, Min, 0.3Q)

\end{figure}

\newpage

\subsection{States tuple}
In our second design, we rely on the idea that the initial convolutional part of the network needs to learn how to extract state feature representation that hopefully could be reused for previous states without significant modifications.
Therefore, instead of modifying the state representation as we did in the previous case, we always store three previous lossless state representations without any changes and we slightly modify the network architecture.
The input of the network will now consist of a tuple of four most recent states, including the current state.

For all states of the tuple, the state representation is passed into the shared convolutional block of the same architecture as before.
However, we add an additional shared small dense layer after the convolutional block, which is applied to each part that corresponds to one of the previous states.
Only then are all four vector parts concatenated back together before being passed to the shared hidden layers as before.

The size of the additional dense layer is intentionally smaller.
First, the idea is to extract only those features that, when combined with the same features from other previous states, can construct the temporal information.
Second, we want to limit the throughput of global state information through this part of the computational graph.

Note that this additional layer is not applied to the part of the tuple input corresponding to the current state.
Therefore, this part of the computation graph is still responsible for computing the overall complex state representation as before.

Note also that both the convolutional block and the additional dense layer are shared for different parts of the input.
The goal is not to train the modules responsible for a particular task multiple times.
The convolutional block should be able to learn such general feature extraction that should be invariant to the particular single input state.
Similarly, we assume that the additional features of the dense layer should be time-invariant, and the construction of the temporal features should be the responsibility of the following dense layer working with concatenated results.

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FSTupleOrderedAvg.png}

    \caption{Forced coordination: FS Tuple, Average, ordered averaged cross-play of population final agents vs self-play agents}
    \label{FSTupleOrderedAvg}
    \medskip
    \small 
    Average cummulative reward of the best final agent using tuple frame stacking extension for temporal features.    
    (Average, 0.25Q, 0.75Q)

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FSTupleOrderedQ15.png}

    \caption{Forced coordination: FS Tuple, 0.15 quantile, ordered averaged cross-play of population final agents vs self-play agents}
    \label{FSTupleOrderedQ15}
    \medskip
    \small 
    Average cummulative reward of the best final agent using tuple frame stacking extension for temporal features.    
    (0.15Q, Min, 0.3Q)

\end{figure}

\subsection{Comparison}
After implementing these temporal features modifications, we performed the same series of diversity population building experiments as before with the simple single-state convolutional representation.
Trained population agents are always evaluated against self-play agents trained with the same frame stacking modifications, eliminating any potential advantages.
In addition, in the initial phase of the experiments, the diversity combination R$1$L$1$ (see Table \ref{tab:KLDiv-coefiicents}) was both problematic in terms of learning, as the training diverged more often than in other settings, and also achieved significantly worse cooperation results (see evaluations \ref{SimpleCNNOrderedAvg}, \ref{SimpleCNNOrderedAvg0.15Q}).
Therefore, for the following experiments we added another slightly weaker combination of parameters R$0$L$0$.

Using the same evaluation scheme as before (see Section \ref{OrderedEvaluation}), the ordered evaluation results of the population training did not turn out to be as better for the channels frame stacking technique as it was for the simple CNN approach (see Figure \ref{FSChannelsOrderedAvg}).
Similarly, the improvement is debatable for the tuple frame stacking variant (Figure \ref{FSTupleOrderedAvg}).
Nevertheless, we argue that for several experimental settings (No divers, R$0$, R$2$, and L$0$) the improvements can be demonstrated at lower values of the average cumulative reward.

However, it is important to mention the comparison of the different sets of self-play agents (Figure \ref{FSVariantsOrderedAvg}).
\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FSVariantsOrderedAvg.png}

    \caption{Forced coordination: Average, ordered averaged cross-play of self-play agents (Simple CNN, FS Channels, FS Tuple)}
    \label{FSVariantsOrderedAvg}
    \medskip
    \small 
    Average cummulative reward of all agents from evaluation set.
    All three types of self-play agent designs included.
    (Average, 0.25Q, 0.75Q)

\end{figure}
This comparison shows that the self-play agents using temporal features managed to cooperate at a lower level of cooperation with a significantly larger set of agents.
On the one hand, this is a great result, since with these extensions even simple self-play agents became better at cooperating.
On the other hand, the improvement in cooperation seems to be less noticeable in our population training.

While the results seem rather bland for the average value of the ordered scores, it still holds that the population training approach shows significant improvement when considering the $0.15$ quantile value.
Here (Figures \ref{FSChannelsOrderedQ15}, \ref{FSTupleOrderedQ15}) the population agents managed to vastly outperform the self-play agents.



\section{Population evaluation}
So far, we have measured how well our trained population agents perform when paired with the reference set of self-play agents and compared this measure to how well self-play agents perform within its same reference set.
However, this evaluation approach may favor self-play agents because they have the theoretical advantage of being trained with the same learning technique.

To make the comparison of these two training methods more fair, instead of evaluating final population agents against different self-play agents, we will now evaluate them against other final population agents from different diversity experiments.
In previous experiments, we considered the best of the two final population agents according to their cross-play results with the self-play set.
However, in the vast majority of cases, the best agent was the one that was trained with the agent training set, which included all agents from the population initialization.
Therefore, for the following evaluation we will always consider the first of the two final agents.

Our evaluated set in the following experiment will consist of one final agent for each diverse population experiment configuration and for each seed, giving us a total of 27 agents for frame stacking extensions (and 40 for original simple CNN agents).
However we will omit such pairs from different configurations that started with same population inicialization agents as these might have significant advantage in common experience. 
We compute the cross-play evaluation matrix for all pairs and group the results by the rows corresponding to the same diversity experiment. 
We then order these results in the same way as in the previous experiments.

With this evaluation setting, our population-trained agents were able to significantly outperform the cooperation performance of the self-play agents when using both the original simple convolutional and channel frame stacking representations (Figures \ref{FinalPopFinalPopSimpleCnnQ15}, \ref{FinalPopFinalPopFsChannelsQ15}).
The most tremendous improvement can be seen on the channels frame stacking variant, where some, albeit very low, level of cooperation was achieved with almost all paired agents, but also in many experiment settings the maximum common outcome of the best pairs was almost comparable to that of the specialized self-play agents.

Unfortunately, the results were not as convincing for the tuple temporal extensions (Figure \ref{FinalPopFinalPopFsTupleQ15}).


\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FinalPopFinalPopSimpleCnnQ15.png}

    \caption{Forced coordination: Simple CNN, 0.15 quantile, ordered averaged cross-play of population final agents}
    \label{FinalPopFinalPopSimpleCnnQ15}
    \medskip
    \small 
    Diversity experiments final population agents trained with simple convolutional state representation against final population agents from different experiments.
    (0.15Q, Min, 0.3Q)

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FinalPopFinalPopFsChannelsQ15.png}

    \caption{Forced coordination: FS Channels, 0.15 quantile, ordered averaged cross-play of population final agents}
    \label{FinalPopFinalPopFsChannelsQ15}
    \medskip
    \small 
    Diversity experiments final population agents trained with channel frame stacking representation against final population agents from different experiments.
    (0.15Q, Min, 0.3Q)

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FinalPopFinalPopFsTupleQ15.png}

    \caption{Forced coordination: FS Tuple, 0.15 quantile, ordered averaged cross-play of population final agents}
    \label{FinalPopFinalPopFsTupleQ15}
    \medskip
    \small 
    Diversity experiments final population agents trained with tuple frame stacking representation against final population agents from different experiments.
    (0.15Q, Min, 0.3Q)


\end{figure}

\newpage

\section{Other layouts}
So far, all of our observations and experiments have revolved around the same Forced Coordination layout.
In the following section, we examine the smaller set of similar experiments on two other layouts.
First, we demonstrate the difference in the cooperation of self-playing agents with respect to temporal features.
And second, we try to apply our diversity population building method.



\subsection{Cramped Room}
On this layout, we can see (Figure \ref{CrampedRoomFSVariantsOrderedAvg}) that adding temporal features using the tuple extension had a positive impact on the overall performance of the self-play agents in terms of the level of cooperation with other agents.
However, channels temporal features did not provide a significant improvement.

While our diverse population showed great signs of improvement in most of our experiment settings when using the channels frame stacking representation (Figure \ref{CrampedRoomFSChannelsOrdered15Q}), there was no overall improvement for the tuple frame stacking representation (Figure \ref{CrampedRoomFSTupleOrdered15Q}), where most of the experiment settings even showed worse performance.
The only exception was the experiment variant R$1$L$1$, which showed a great improvement in terms of cooperation with a larger set of agents. 

However, the problem with this layout, as discussed earlier, is that the failure of pairwise cooperation between different self-play agents is almost nonexistent.
In our evaluation set of self-play agents, there were few agents that managed to cooperate with all other agents in the set at a high level of cooperation.
We hypothesize that our diversity approach will not show much improvement on layouts where the cooperation problem does not exist in the first place.



\begin{figure}[!ht]
    \centering
    \includegraphics*[width=8cm]{../img/CrampedRoomFSVariantsOrderedAvg.png}

    \caption{Cramped room: Average, ordered averaged cross-play of self-play agents (Simple CNN, FS Channels, FS Tuple)}
    \label{CrampedRoomFSVariantsOrderedAvg}
    \medskip
    \small 
    Average cummulative reward on the Cramped room layout of all self-play agents.
    All three types of self-play agent designs included.

    (Average, 0.25Q, 0.75Q)

\end{figure}


\begin{figure}[!ht]
    \centering
    \includegraphics*[width=9cm]{../img/CrampedRoomFSChannelsOrdered15Q.png}

    \caption{Cramped room: FS Channels, 0.15 quantile, ordered averaged cross-play of population final agents}
    \label{CrampedRoomFSChannelsOrdered15Q}
    \medskip
    \small 
    Average cummulative reward on the Cramped room layout of the best final agent using channels extension for temporal features.    
    (0.15Q, Min, 0.3Q)

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=9cm]{../img/CrampedRoomFSTupleOrdered15Q.png}

    \caption{Cramped room: FS Tuple, 0.15 quantile, ordered averaged cross-play of population final agents}
    \label{CrampedRoomFSTupleOrdered15Q}
    \medskip
    \small 
    Average cummulative reward on the Cramped room layout of the best final agents evaluated against self-play agents using tuple frame stacking for temporal features.
    (0.15Q, Min, 0.3Q)

\end{figure}

\newpage

\subsection{Counter Circuit}
The application of temporal features had a significant impact on the Counter Circuit layout(Figure \ref{CounterCircuitFSVariantsOrderedAvg}) with both temporal features variants.
Interestingly, the evaluation curve of channels and tuple is different. 
It seems that the use of the channels frame stacking representation here causes agents to cooperate with virtually all other agents at a non-significantly low level of cooperation, while at some point the increase in cooperation level is outperformed by a states tuple representation.

On this layout, the diversity population method managed to outperform the self-play agents in most of the experiment settings (Figures \ref {CounterCircuitFSChannelsOrderedQ15}, \ref {CounterCircuitFSTupleOrderedQ15}).
However, in both variants the cooperation failed noticeably on two particular settings. 


\begin{figure}[!ht]
    \centering
    \includegraphics*[width=8cm]{../img/CounterCircuitFSVariantsOrderedAvg.png}

    \caption{Counter circuit: Average, ordered averaged cross-play of self-play agents (Simple CNN, FS Channels, FS Tuple)}
    \label{CounterCircuitFSVariantsOrderedAvg}
    \medskip
    \small 
    Average cummulative reward on the Counter circuit layout of all self-play agents.
    All three types of self-play agent designs included.

    (Average, 0.25Q, 0.75Q)

\end{figure}


\begin{figure}[!ht]
    \centering
    \includegraphics*[width=8cm]{../img/CounterCircuitFSChannelsOrderedQ15.png}

    \caption{Counter circuit: FS Channels, 0.15 quantile, ordered averaged cross-play of population final agents}
    \label{CounterCircuitFSChannelsOrderedQ15}
    \medskip
    \small 
    Average cummulative reward on the Counter circuit layout of the best final agent using channels extension for temporal features.    
    (0.15Q, Min, 0.3Q)

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=8cm]{../img/CounterCircuitFSTupleOrderedQ15.png}

    \caption{Counter circuit: FS Tuple, 0.15 quantile, ordered averaged cross-play of population final agents}
    \label{CounterCircuitFSTupleOrderedQ15}
    \medskip
    \small 
    Average cummulative reward on the Counter circuit layout of the best final agents evaluated against self-play agents using tuple frame stacking for temporal features.
    (0.15Q, Min, 0.3Q)

\end{figure}



\section{Results remarks}

After conducting multiple experiments on several different kitchen layouts, we try to summarize the resulting observations.

The main improvement in cooperative capabilities was the use of temporal features.
In our evaluation experiments, even a self-play trained agent showed great improvement with other self-play agents when extended by recent state frames information.
We believe this result is intuitive, since even during self-play training, due to the stochastic policy sampling, agents may experience different sub-trajectories leading to similarly good results.
If they have several previous states information available, they should be better able to predict their partner's current subgoal.

We believe that our idea of incrementally building diverse populations has slightly improved the majority of our experiments.
However, as we discussed in the introductory chapters (Section \ref{RobustnessEvaluation}), choosing the right method for cooperation evaluation is another problem in itself.

For us, the chosen condition of the cooperation measure was to look at the order of the pairwise achieved common results and to consider the worse case (0.15 quantile value).
Under this condition, the majority of our experiments exhibited significant coordination improvement at lower cooperation levels.
Agents trained with our proposed diverse population generation process were able to cooperate with a much larger set of other agents, albeit at the cost of a significantly reduced maximum outcome.
To make a final judgment about this finding, the results must be confronted with the desired definition of cooperation.
In our opinion, producing agents capable of some reasonable level of cooperation with a larger set of unknown agents is a step in the right direction.
And the fact that our results showed this form of cooperation ability in 0.15 quantile of all evaluations performed indicates the robustness of our method.


While our population building approach showed on average an improvement in most of the experiment settings, it is questionable what exact experiment configuration should be considered as the most appropriate one, as for all of them we can find some of the evaluations performed where the results were not very convincing.
We try to display the overall results in the following table (Table \ref{tab:exp_settings_results}), where we summarize all 0.15 quantile subresults from previous experiments (Figures \ref{SimpleCNNOrderedAvg0.15Q}, \ref{FSChannelsOrderedQ15}, \ref{FSTupleOrderedQ15}, \ref{CrampedRoomFSChannelsOrdered15Q}, \ref{CrampedRoomFSTupleOrdered15Q}, \ref{CounterCircuitFSChannelsOrderedQ15}, \ref{CounterCircuitFSTupleOrderedQ15}, \ref{FinalPopFinalPopSimpleCnnQ15} \ref{FinalPopFinalPopFsChannelsQ15} \ref{FinalPopFinalPopFsTupleQ15}).
\begin{table}[tbp]
    \small
    \centering
    \begin{tabular}{l|llllll|l}
      \toprule
      Experiment Name        & $0.5$     & $0.6$  & $0.7$  & $0.8$  & $0.9$  & $1.0$ & Average          \\ \midrule
      SP           & $0.066$ & $0.08$ & $0.103$ & $0.193$ & $0.364$ & $0.774$ & $0.263$   \\\midrule      

      No divers.   & $0.182$ & $0.198$ & $0.312$ & $0.417$ & $0.441$ & $0.592$ & $0.357$ \\\midrule
      R$0$         & $0.16$ & $0.232$ & $0.335$ & $0.371$ & $0.43$ & $0.566$ & $0.349$ \\
      R$1$         & $0.142$ & $0.207$ & $0.283$ & $0.386$ & $0.454$ & $0.559$ & $0.338$ \\
      R$2$         & $0.127$ & $0.2$ & $0.287$ & $0.405$ & $0.451$ & $0.572$ & $0.34$ \\\midrule
      L$0$         & $0.164$ & $0.255$ & $0.324$ & $0.396$ & $0.484$ & $0.654$ & $0.38$  \\
      L$1$         & $0.111$ & $0.169$ & $0.252$ & $0.296$ & $0.382$ & $0.482$ & $0.282$  \\
      L$2$         & $0.155$ & $0.204$ & $0.265$ & $0.369$ & $0.386$ & $0.536$ & $0.319$  \\\midrule
      R$0$L$0$     & $0.144$ & $0.199$ & $0.249$ & $0.311$ & $0.386$ & $0.532$ & $0.303$  \\
      R$1$L$1$     & $0.117$ & $0.167$ & $0.202$ & $0.289$ & $0.376$ & $0.54$ & $0.282$  \\
      
      
     \bottomrule
    \end{tabular}
    \caption{Summary of results across all performed experiments}
    \label{tab:exp_settings_results}
    \small
    This table shows results at interesting points (50\%, 60\%, 70\%, 80\%, 90\%, 100\%) of all ordered cross-play evaluations.    
\end{table}
We examine interesting points of the ordered scores (50\%, 60\%, 70\%, 80\%, 90\%, 100\%) where the difference between self-play and population agents is most visible.
Since the sub-results originate from different layouts where the maximum possible outcome value also differs, we try to normalize them by dividing the values of the sub-results by the maximum outcome obtained by self-play agents on each corresponding layout.

Examining the results in the table, we can make some further observations.
As discussed earlier, in all experiment settings our population of final agents outperform the self-play agents in terms of cooperation with up to 90\% of the evaluation agents, albeit at a lower level of cooperation.

From the point of view of specific KL Divergence diversification, the worst results were found when both reward and goal were increased (R$0$L$0$, R$1$L$1$).
It seems that using both diversifying pressures for a given parameter sizes has a rather negative effect on the overall cooperation.
If we focus on each of the diversifying techniques separately, the results are less consistent.
It often happens that the order of results of two particular settings is different when looking at two different evaluation points, which makes it difficult to formulate a final statement about the findings.

However, in our opinion, all three variants of reward augmentation yielded more stable results with smaller deviations.
Better results at early stage of evaluation (50\%, 60\%) are obtained with weaker variant (R$0$), while the stronger variants (R$1$ and R$2$) seem to perform better later (80\%, 90\%).


More deviations were shown in experiments where the KL divergence term was used in the objective function.
The first and third variants (L$0$, L$2$) performed reasonably well, while the second variant (L$1$) seemed to fail at many evaluation points.
However, the relationship between the proposed variants of the loss augmentation experiment is a somewhat more complicated one, since the ratio between the loss coefficient and the loss clipping varies a lot. 
It is difficult to say if this variation is just caused by some statistical error or if this particular setting is actually so detrimental to the overall cooperation.

However, while it may seem that overall the best results were obtained when no diversification was used, if we look at almost any particular experiment performed, we can find some specialized diversification variant that worked better.
For each of the reward augmentation variants we can find evidence of better performance than the no diversification variant in the following experiments (Figures \ref{SimpleCNNOrderedAvg}, \ref{SimpleCNNOrderedAvg0.15Q}, \ref{FSChannelsOrderedAvg}, \ref{FSChannelsOrderedQ15}, \ref{CounterCircuitFSChannelsOrderedQ15}).
Additionally, for a reward augmentation variant R$1$, we see an extraordinary improvement with the simple convolutional state representation when our final agents were evaluated against each other (Figure \ref{FinalPopFinalPopSimpleCnnQ15}). 

Similarly, for each objective term augmentation variant, we can find cases where the results were better (Figure \ref{FSChannelsOrderedAvg}, \ref{FSChannelsOrderedQ15}, \ref{FSTupleOrderedQ15}, \ref{FinalPopFinalPopFsChannelsQ15}, \ref{FinalPopFinalPopFsTupleQ15}, \ref{CrampedRoomFSChannelsOrdered15Q}, \ref{CounterCircuitFSChannelsOrderedQ15}).

Interestingly, while the strongest diversification variant R$1$L$1$ has the worst result in terms of table results (Table \ref{tab:exp_settings_results}), it was the only variant that managed to outperform the diversity-free variant on the Cramped Room layout using tuple temporal information (Figure \ref{CrampedRoomFSTupleOrdered15Q}).


Nevertheless, it appears that even the configuration with no diversity pressure, where the only source of diversity is stochastic sampling of the trajectories of previous partners, performs relatively well.
In all of our previous evaluations, we have mainly focused on the number of evaluation agents we are able to cooperate with, while ignoring the question of cooperation diversity.
However, one of our original motivations for diverse population building was to discover new diverse behaviors that could lead to cooperation with agents with different types of behavior.
If we take a step back to our initial trials (Figure \ref{CNNPopNoDiffBestInitFixation}), we can see that this has indeed been partially achieved, even in settings where no other diversification is employed.
On all but the first seed we can find evidence of agents that were able to cooperate with some of the evaluation self-play agents that none from the population initialization could.
Unfortunately, this does not occur as often as we would hope, and such new behavior is rarely captured in the behavior of the final population agent.  