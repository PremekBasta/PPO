\chapter*{Conclusion}

We began this thesis with a brief introduction to reinforcement learning and its extension to multi-agent environments (Chapters \ref{IntroductionChapter}, \ref{RLChapter}, and \ref{MAS}).

We then proceeded to provide a description of a simplified fully cooperative multi-agent cooking game environment (Chapter \ref{OvercookedChapter}), which we worked with in the rest of our thesis to study the cooperative capabilities of trained agents. 
Subsequently, we tried to reimplement some results based on related previous work (Chapters \ref{RelatedWorkChapter} and \ref{BackgroundChapter}), where we tried to demonstrate allegedly poor cooperative abilities of self-play trained agents, which later became a solid baseline for our experiments.

Later, we introduced our own novel iterative approach of the diverse population method (Chapter \ref{DiversePopulationMethodChapter}).
Here we tried to simplify the multi-agent environment towards a single-agent perspective by effectively introducing domain randomization technique.
Apart from this randomization, we tried to induce even more diversity into the population by incorporating KL divergence bonuses to enhance the differences between the behaviors of the population agents.
With such designed settings, we conducted several different experiments to evaluate the cooperative capabilities and robustness of such a method.

Here we revisited the problem of cooperation evaluation, where we demonstrated that considering only an overall mean is not a sufficient metric.
Instead, we proposed to first perform a cross-play evaluation of the evaluated agents against the evaluation set, and then to arrange these results in an ascending order, and to study the whole progression of such an order.

With the addition of considering the lower 0.15 quantile of results, we were able to meet our initial goals and demonstrate robust cooperation with a much larger set of agents compared to self-play agents.
However, while our agents were able to cooperate with a considerably broader set of self-play agents, this came at the cost of a lower yet still reasonable level of cooperation, as our agents were not able to achieve as high outcomes as self-play agents.
Nevertheless, we believe that such a shift can be seen as an improvement in cooperation abilities, since we assume that it is more desirable to have an agent capable of reasonable cooperation with much wider types of agent behaviors than to have a specialized agent that can only achieve excellent results with limited types of partner behaviors.


We were then able to slightly improve the cooperation of the agents by extending the information available to the agents by incorporating the temporal information about previous states of the environment.
After proposing two variants of such extension, we evaluated different agents trained with our method against each other, where in combination of temporal information extension our agents showed tremendous results both in cooperating with a wide range of other agents and also managed to come close to the maximum result similar to that of self-play agents.
With this result, we claim that our method shows an overall improvement in robustness over self-play training.

After applying all of these ideas and improvements to one particular layout where we were able to get an apparent improvement in cooperation, we tried to evaluate our method on two other layouts.
The improvements on these layouts were not as significant as we had hoped, especially on the Cramped Room layout, which seems to be a layout where agents have no evident problem with pairwise cooperation.
On this layout, even using only tuple temporal features made the agents significantly more cooperative, and our diverse method was able to improve on this achieved cooperation in only one particular configuration.

While we believe that the idea of using KL divergence as a tool to promote diversity is a useful one, since almost all configurations were the dominant ones in some experiments, when we attempted to evaluate all the experiments performed, we were unable to derive generally appropriate parameters for such diversification.
In future work, more experiments could be conducted in different environments to further infer the effects of specific diversity configurations.

Nevertheless, we conclude by claiming that our iterative population-building process showed some improvement in cooperation even when used without applying any additional diversity-promoting tools.
Therefore, we believe that even simple domain randomization by sampling previous stochastic population partners alone is a reasonable diversification technique after all.

After summarizing all of our results, we suggest that future work could further investigate the effect of better population initialization, as we were limited to only three somewhat diverse agent behaviors.
In addition, it seemed to us that the success of our approach was more dependent on the diversity of the initial population than we expected.
Similarly, we suggest that it might be beneficial to focus more on prioritizing sampled population partners to force even more robust cooperation with as many partner behaviors as possible, since in our experiments we only sampled our partners uniformly.


\addcontentsline{toc}{chapter}{Conclusion}
