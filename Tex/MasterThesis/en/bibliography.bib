%
%  An example of a bibliographical database for bibTeX
%
%  Recommended software for maintenance of *.bib files:
%    JabRef, http://jabref.sourceforge.net/
%
%  BEWARE:
%
%    *  If a name contains a capital letter, which must be kept such,
%       use curly brackets ({T}hailand, {HIV}).
%
%  ===========================================================================

@BOOK{Andel07,
  title = {Základy matematické statistiky},
  publisher = {Matfyzpress},
  year = {2007},
  author = {Anděl, J.},
  address = {Praha},
  series = {Druhé opravené vydání},
  isbn = {80-7378-001-1}
}

@BOOK{Andel98,
  title = {Statistické metody},
  publisher = {Matfyzpress},
  year = {1998},
  author = {Anděl, J.},
  address = {Praha},
  series = {Druhé přepracované vydání},
  isbn = {80-85863-27-8}
}

@ARTICLE{Cox72,
  author = {Cox, D. R.},
  title = {Regression models and life-tables (with {D}iscussion)},
  journal = {Journal of the Royal Statistical Society, Series B},
  year = {1972},
  volume = {34},
  pages = {187--220},
  number = {2}
}

@ARTICLE{DempsterLairdRubin77,
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  title = {Maximum likelihood from incomplete data via the {EM} algorithm},
  journal = {Journal of the Royal Statistical Society, Series B},
  year = {1977},
  volume = {39},
  pages = {1--38},
  number = {1}
}

@ARTICLE{Genberget08,
  author = {Genberg, B. L. and Kulich, M. and Kawichai, S. and Modiba, P. and
	Chingono, A. and Kilonzo, G. P. and Richter, L. and Pettifor, A.
	and Sweat, M. and Celentano, D. D.},
  title = {{HIV} risk behaviors in Sub-{S}aharan {A}frica and {N}orthern {T}hailand:
	{B}aseline behavioral data from project {A}ccept},
  journal = {Journal of Acquired Immune Deficiency Syndrome},
  year = {2008},
  volume = {49},
  pages = {309--319}
}

@ARTICLE{KaplanMeier58,
  author = {Kaplan, E. L. and Meier, P.},
  title = {Nonparametric estimation from incomplete observations},
  journal = {Journal of the American Statistical Association},
  year = {1958},
  volume = {53},
  pages = {457--481},
  number = {282}
}

@BOOK{LehmannCasella98,
  title = {Theory of Point Estimation},
  publisher = {Springer-Verlag},
  year = {1998},
  author = {Lehmann, E. L. and Casella, G.},
  address = {New York},
  series = {{S}econd {E}dition},
  isbn = {0-387-98502-6}
}

@ARTICLE{Student08,
  author = {Student},
  title = {On the probable error of the mean},
  journal = {Biometrika},
  year = {1908},
  volume = {6},
  pages = {1-25}
}


@url{DotaOpenFive,
  author          = {OpenAI},
  journal         = {OpenAI Five finals. },
  number          = {},
  title           = {},
  volume          = {},
  year            = {2019},
  url             = {https://openai.com/blog/openai-five-finals/},
}


@url{SpinningUpIntro,
  author          = {Josh Achiam},
  journal         = {},
  number          = {},
  title           = {},
  volume          = {},
  year            = {2018},
  url             = {https://spinningup.openai.com/en/latest/},
}


@misc{liang2018rllib,
      title={RLlib: Abstractions for Distributed Reinforcement Learning}, 
      author={Eric Liang and Richard Liaw and Philipp Moritz and Robert Nishihara and Roy Fox and Ken Goldberg and Joseph E. Gonzalez and Michael I. Jordan and Ion Stoica},
      year={2018},
      eprint={1712.09381},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1712.09381},
}


@misc{stable-baselines,
  author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {Stable Baselines},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/hill-a/stable-baselines}},
}






@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
} 

@phdthesis{Watkins:89,
  added-at = {2008-03-11T14:52:34.000+0100},
  author = {Watkins, C. J. C. H.},
  biburl = {https://www.bibsonomy.org/bibtex/21ffd549077ea1da7675431a17fa2af03/idsia},
  citeulike-article-id = {2381652},
  interhash = {ca824d64b71939208358edb4a26f8351},
  intrahash = {1ffd549077ea1da7675431a17fa2af03},
  keywords = {juergen},
  priority = {2},
  school = {King's College, Oxford},
  timestamp = {2008-03-11T14:53:51.000+0100},
  title = {Learning from Delayed Rewards},
  year = 1989
}



@article{Atari,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei and Veness, Joel and Bellemare, Marc and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
year = {2015},
month = {02},
pages = {529-33},
title = {Human-level control through deep reinforcement learning},
volume = {518},
journal = {Nature},
doi = {10.1038/nature14236}
}

@misc{Rainbow,
  doi = {10.48550/ARXIV.1710.02298},
  
  url = {https://arxiv.org/abs/1710.02298},
  
  author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{GAE,
  doi = {10.48550/ARXIV.1506.02438},
  
  url = {https://arxiv.org/abs/1506.02438},
  
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  
  keywords = {Machine Learning (cs.LG), Robotics (cs.RO), Systems and Control (eess.SY), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{SpinningUp2018,
    author = {Achiam, Joshua},
    title = {{Spinning Up in Deep Reinforcement Learning}},
    year = {2018}
}

@misc{TRPO,
  doi = {10.48550/ARXIV.1502.05477},
  
  url = {https://arxiv.org/abs/1502.05477},
  
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Trust Region Policy Optimization},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@book{KLDIV,
  added-at = {2008-09-16T23:39:07.000+0200},
  address = {New York},
  author = {Kullback, Solomon},
  biburl = {https://www.bibsonomy.org/bibtex/28d0af9cdd06af73190b01cc1e04da70b/brian.mingus},
  booktitle = {Information Theory and Statistics},
  description = {CCNLab BibTeX},
  interhash = {03b56ca50da39d05c8832fb6f814ddda},
  intrahash = {8d0af9cdd06af73190b01cc1e04da70b},
  keywords = {stats},
  publisher = {Wiley},
  timestamp = {2008-09-16T23:40:28.000+0200},
  title = {Information Theory and Statistics},
  year = 1959
}


@book{LagrangDuality,
  added-at = {2008-03-02T02:12:02.000+0100},
  address = {Princeton, N. J.},
  author = {Rockafellar, R. Tyrrell},
  biburl = {https://www.bibsonomy.org/bibtex/223aa07ea525f6dd11585fc2037a0daf1/dmartins},
  callnumber = {UniM Maths 516.08 R59},
  description = {robotica-bib},
  interhash = {30830becb0a2c5ebca5946b895d9740a},
  intrahash = {23aa07ea525f6dd11585fc2037a0daf1},
  keywords = {imported},
  notes = {A SRL reference.},
  publisher = {Princeton University Press},
  series = {Princeton Mathematical Series},
  timestamp = {2008-03-02T02:14:11.000+0100},
  title = {Convex analysis},
  year = 1970,
  part= {VI},
}

@article{BacktrackingLineSearch,
author = {Larry Armijo},
title = {{Minimization of functions having Lipschitz continuous first partial derivatives.}},
volume = {16},
journal = {Pacific Journal of Mathematics},
number = {1},
publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
pages = {1 -- 3},
year = {1966},
doi = {pjm/1102995080},
URL = {https://doi.org/}
}

@misc{PPO,
  doi = {10.48550/ARXIV.1707.06347},
  
  url = {https://arxiv.org/abs/1707.06347},
  
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Proximal Policy Optimization Algorithms},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{CPI,
  title={Approximately Optimal Approximate Reinforcement Learning},
  author={Sham M. Kakade and John Langford},
  booktitle={International Conference on Machine Learning},
  year={2002}
}

@article{EntropyRegularization,
author = {Williams, Ronald J.},
title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
issn = {0885-6125},
url = {https://doi.org/10.1007/BF00992696},
doi = {10.1007/BF00992696},
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
journal = {Mach. Learn.},
month = {may},
pages = {229–256},
numpages = {28},
keywords = {gradient descent, mathematical analysis, Reinforcement learning, connectionist networks}
}

@misc{DDPG,
  doi = {10.48550/ARXIV.1509.02971},
  
  url = {https://arxiv.org/abs/1509.02971},
  
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Continuous control with deep reinforcement learning},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{TD3,
  doi = {10.48550/ARXIV.1802.09477},
  
  url = {https://arxiv.org/abs/1802.09477},
  
  author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Addressing Function Approximation Error in Actor-Critic Methods},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{SAC,
  doi = {10.48550/ARXIV.1812.05905},
  
  url = {https://arxiv.org/abs/1812.05905},
  
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Robotics (cs.RO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Soft Actor-Critic Algorithms and Applications},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{MMDP,
author = {Boutilier, Craig},
title = {Planning, Learning and Coordination in Multiagent Decision Processes},
year = {1996},
isbn = {1558604179},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {There has been a growing interest in AI in the design of multiagent systems, especially in multiagent cooperative planning. In this paper, we investigate the extent to which methods from single-agent planning and learning can be applied in multiagent settings. We survey a number of different techniques from decision-theoretic planning and reinforcement learning and describe a number of interesting issues that arise with regard to coordinating the policies of individual agents. To this end, we describe <i>multiagent Markov decision processes</i> as a general model in which to frame this discussion. These are special <i>n-</i>person cooperative games in which agents share the same utility function. We discuss coordination mechanisms based on imposed conventions (or social laws) as well as learning methods for coordination. Our focus is on the decomposition of sequential decision processes so that coordination can be learned (or imposed) locally, at the level of individual states. We also discuss the use of structured problem representations and their role in the generalization of learned conventions and in approximation.},
booktitle = {Proceedings of the 6th Conference on Theoretical Aspects of Rationality and Knowledge},
pages = {195–210},
numpages = {16},
location = {The Netherlands},
series = {TARK '96}
}

@article{NASH,
author = {John F. Nash },
title = {Equilibrium points in n-person games},
journal = {Proceedings of the National Academy of Sciences},
volume = {36},
number = {1},
pages = {48-49},
year = {1950},
doi = {10.1073/pnas.36.1.48},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.36.1.48},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.36.1.48}}

@article{DecPOMDP,
	doi = {10.1613/jair.2447},
  
	url = {https://doi.org/10.1613%2Fjair.2447},
  
	year = 2008,
	month = {may},
  
	publisher = {{AI} Access Foundation},
  
	volume = {32},
  
	pages = {289--353},
  
	author = {F. A. Oliehoek and M. T. J. Spaan and N. Vlassis},
  
	title = {Optimal and Approximate Q-value Functions for Decentralized {POMDPs}
},
  
	journal = {Journal of Artificial Intelligence Research}
}

@misc{MADDPG,
  doi = {10.48550/ARXIV.1706.02275},
  
  url = {https://arxiv.org/abs/1706.02275},
  
  author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{MAPPO,
  doi = {10.48550/ARXIV.2103.01955},
  
  url = {https://arxiv.org/abs/2103.01955},
  
  author = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Multiagent Systems (cs.MA), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{StochasticGame,
author = {L. S. Shapley },
title = {Stochastic Games*},
journal = {Proceedings of the National Academy of Sciences},
volume = {39},
number = {10},
pages = {1095-1100},
year = {1953},
doi = {10.1073/pnas.39.10.1095},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.39.10.1095},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.39.10.1095}}


@url{Starcraft,
  author          = {Deepmind},
  journal         = {AlphaStar: Mastering the real-time strategy game StarCraft II},
  number          = {},
  title           = {},
  volume          = {},
  year            = {2019},
  url             = {https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii},
}

@misc{carroll2020utility,
      title={On the Utility of Learning about Humans for Human-AI Coordination}, 
      author={Micah Carroll and Rohin Shah and Mark K. Ho and Thomas L. Griffiths and Sanjit A. Seshia and Pieter Abbeel and Anca Dragan},
      year={2020},
      eprint={1910.05789},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.05789}
}



@inproceedings{Ho2016GenerativeAI,
 author = {Ho, Jonathan and Ermon, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Imitation Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/cc7e2b878868cbae992d1fb743995d8f-Paper.pdf},
 volume = {29},
 year = {2016}
}



@inproceedings{10.1007/978-3-030-63823-8_46,
title = "Investigating Partner Diversification Methods in Cooperative Multi-agent Deep Reinforcement Learning",
abstract = "Overfitting to learning partners is a known problem, in multi-agent reinforcement learning (MARL), due to the co-evolution of learning agents. Previous works explicitly add diversity to learning partners for mitigating this problem. However, since there are many approaches for introducing diversity, it is not clear which one should be used under what circumstances. In this work, we clarify the situation and reveal that widely used methods such as partner sampling and population-based training are unreliable at introducing diversity under fully cooperative multi-agent Markov decision process. We find that generating pre-trained partners is a simple yet effective procedure to achieve diversity. Finally, we highlight the impact of diversified learning partners on the generalization of learning agents using cross-play and ad-hoc team performance as evaluation metrics.",
keywords = "Coordination, Deep reinforcement learning, Generalization, Multi-agent system",
author = "Rujikorn Charakorn and Poramate Manoonpong and Nat Dilokthanakul",
year = "2020",
doi = "10.1007/978-3-030-63823-8_46",
language = "English",
isbn = "9783030638221",
volume = "5",
series = "Communications in Computer and Information Science",
publisher = "Springer Science+Business Media",
pages = "395--402",
editor = "Haiqin Yang and Kitsuchart Pasupa and Leung, {Andrew Chi-Sing} and Kwok, {James T.} and Chan, {Jonathan H.} and Irwin King",
booktitle = "Neural Information Processing. 27th International Conference, ICONIP 2020, Bangkok, Thailand, November 18–22, 2020, Proceedings, Part V",
address = "United States",
note = "27th International Conference on Neural Information Processing, ICONIP 2020 ; Conference date: 18-11-2020 Through 22-11-2020",

}





@misc{knott2021evaluating,
      title={Evaluating the Robustness of Collaborative Agents}, 
      author={Paul Knott and Micah Carroll and Sam Devlin and Kamil Ciosek and Katja Hofmann and A. D. Dragan and Rohin Shah},
      year={2021},
      eprint={2101.05507},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.05507}
}

@url{OvercookedGame,
  author          = {Ghost Town Games},
  journal         = {},
  number          = {},
  title           = {},
  volume          = {},
  year            = {2016},
  url             = {https://ghosttowngames.com/overcooked/},
}

@url{OvercookedImplementation,
  author          = {Micah Carroll},
  journal         = {},
  number          = {},
  title           = {},
  volume          = {},
  year            = {},
  url             = {https://github.com/HumanCompatibleAI/overcooked_ai},
}



@article{KLDivergence,
author = {S. Kullback and R. A. Leibler},
title = {{On Information and Sufficiency}},
volume = {22},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {79 -- 86},
year = {1951},
doi = {10.1214/aoms/1177729694},
URL = {https://doi.org/10.1214/aoms/1177729694}
}